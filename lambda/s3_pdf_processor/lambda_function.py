"""
S3 ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜
S3ì— PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ ë²¡í„°í™” ì²˜ë¦¬

íŠ¸ë¦¬ê±° ì¡°ê±´:
- ì´ë²¤íŠ¸: s3:ObjectCreated:Put, s3:ObjectCreated:Post
- ì ‘ë¯¸ì‚¬: .pdf

í™˜ê²½ ë³€ìˆ˜:
- DB_HOST: RDS í˜¸ìŠ¤íŠ¸
- DB_NAME: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
- DB_USER: DB ì‚¬ìš©ìëª…
- DB_PASSWORD: DB ë¹„ë°€ë²ˆí˜¸
- AWS_REGION: AWS ë¦¬ì „
- API_ENDPOINT: FastAPI ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ (ì˜ˆ: http://ec2-ip:8000)
- DEFAULT_SCHOOL_ID: ê¸°ë³¸ í•™êµ ID
"""

import json
import os
import urllib.parse
import boto3
from io import BytesIO
import psycopg2
from PyPDF2 import PdfReader
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS í´ë¼ì´ì–¸íŠ¸
s3_client = boto3.client('s3')
bedrock_client = boto3.client('bedrock-runtime', region_name=os.getenv('AWS_REGION', 'us-west-1'))


def extract_metadata_from_s3_path(s3_key: str) -> dict:
    """
    S3 ê²½ë¡œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ì˜ˆìƒ ê²½ë¡œ êµ¬ì¡°: documents/{school_id}/{category}/{filename}
    ë˜ëŠ”: documents/{filename}

    Args:
        s3_key: S3 ê°ì²´ í‚¤

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    parts = s3_key.split('/')

    metadata = {
        'school_id': int(os.getenv('DEFAULT_SCHOOL_ID', 1)),
        'category': 'general',
        'department': None
    }

    # ê²½ë¡œ êµ¬ì¡° íŒŒì‹±
    if len(parts) >= 4 and parts[0] == 'documents':
        try:
            metadata['school_id'] = int(parts[1])
            metadata['category'] = parts[2]
        except (ValueError, IndexError):
            pass

    # íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡  (ì„ íƒì )
    filename = parts[-1].lower()
    if 'scholarship' in filename or 'ì¥í•™' in filename:
        metadata['category'] = 'scholarship'
    elif 'academic' in filename or 'í•™ì‚¬' in filename:
        metadata['category'] = 'academic'
    elif 'facility' in filename or 'ì‹œì„¤' in filename:
        metadata['category'] = 'facilities'
    elif 'career' in filename or 'ì§„ë¡œ' in filename or 'ì·¨ì—…' in filename:
        metadata['category'] = 'career'

    return metadata


def extract_text_from_pdf(bucket: str, key: str) -> str:
    """
    S3ì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        bucket: S3 ë²„í‚· ì´ë¦„
        key: S3 ê°ì²´ í‚¤

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    logger.info(f"ğŸ“¥ Downloading PDF from s3://{bucket}/{key}")

    # S3ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
    response = s3_client.get_object(Bucket=bucket, Key=key)
    pdf_stream = BytesIO(response['Body'].read())

    # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    reader = PdfReader(pdf_stream)
    text = ""

    for i, page in enumerate(reader.pages):
        page_text = page.extract_text() or ""
        text += f"[í˜ì´ì§€ {i+1}]\n{page_text}\n\n"

    logger.info(f"âœ… Extracted {len(text)} characters from {len(reader.pages)} pages")
    return text.strip()


def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        text: ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]

        if len(chunk.strip()) > 50:  # ìµœì†Œ 50ì
            chunks.append(chunk.strip())

        start += (chunk_size - chunk_overlap)

    logger.info(f"âœ‚ï¸  Created {len(chunks)} chunks")
    return chunks


def generate_embedding(text: str) -> list:
    """
    Bedrock Titanì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

    Returns:
        ì„ë² ë”© ë²¡í„° (1536ì°¨ì›) ë˜ëŠ” None
    """
    try:
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        if len(text) > 30000:
            text = text[:30000] + "..."

        body = json.dumps({"inputText": text})
        response = bedrock_client.invoke_model(
            body=body,
            modelId="amazon.titan-embed-text-v1",
            accept="application/json",
            contentType="application/json"
        )

        response_body = json.loads(response.get("body").read())
        embedding = response_body.get("embedding")

        return embedding

    except Exception as e:
        logger.error(f"âŒ Embedding generation failed: {e}")
        return None


def save_to_database(
    bucket: str,
    key: str,
    text: str,
    chunks: list,
    metadata: dict
):
    """
    ë¬¸ì„œì™€ ì²­í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        bucket: S3 ë²„í‚·
        key: S3 í‚¤
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        metadata: ë©”íƒ€ë°ì´í„°
    """
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = psycopg2.connect(
        host=os.getenv('DB_HOST'),
        database=os.getenv('DB_NAME'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD'),
        port=5432
    )

    try:
        cur = conn.cursor()

        # S3 URL ìƒì„±
        s3_url = f"https://{bucket}.s3.{os.getenv('AWS_REGION', 'us-west-1')}.amazonaws.com/{key}"
        filename = key.split('/')[-1]

        # 1. Document ë ˆì½”ë“œ ì‚½ì…
        cur.execute("""
            INSERT INTO documents (school_id, category, file_name, source_url, department, created_at)
            VALUES (%s, %s, %s, %s, %s, NOW())
            RETURNING id
        """, (
            metadata['school_id'],
            metadata['category'],
            filename,
            s3_url,
            metadata['department']
        ))

        document_id = cur.fetchone()[0]
        logger.info(f"ğŸ’¾ Document saved with ID: {document_id}")

        # 2. ê° ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± ë° ì €ì¥
        chunk_count = 0
        for i, chunk_text in enumerate(chunks):
            # ì„ë² ë”© ìƒì„±
            embedding = generate_embedding(chunk_text)

            if embedding:
                # pgvector í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                embedding_str = '[' + ','.join(map(str, embedding)) + ']'

                cur.execute("""
                    INSERT INTO document_chunks (document_id, chunk_text, embedding)
                    VALUES (%s, %s, %s::vector)
                """, (document_id, chunk_text, embedding_str))

                chunk_count += 1
            else:
                # ì„ë² ë”© ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì €ì¥ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
                cur.execute("""
                    INSERT INTO document_chunks (document_id, chunk_text, embedding)
                    VALUES (%s, %s, NULL)
                """, (document_id, chunk_text))

                chunk_count += 1

        conn.commit()
        logger.info(f"âœ… Saved {chunk_count} chunks to database")

    except Exception as e:
        conn.rollback()
        logger.error(f"âŒ Database error: {e}")
        raise

    finally:
        cur.close()
        conn.close()


def lambda_handler(event, context):
    """
    Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜
    S3 ì´ë²¤íŠ¸ë¥¼ ë°›ì•„ PDFë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        event: S3 ì´ë²¤íŠ¸
        context: Lambda ì»¨í…ìŠ¤íŠ¸

    Returns:
        ì²˜ë¦¬ ê²°ê³¼
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ S3 PDF ìë™ ë²¡í„°í™” Lambda ì‹œì‘")
    logger.info("=" * 60)

    try:
        # S3 ì´ë²¤íŠ¸ì—ì„œ ë²„í‚·ê³¼ í‚¤ ì¶”ì¶œ
        record = event['Records'][0]
        bucket = record['s3']['bucket']['name']
        key = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')

        logger.info(f"ğŸ“‚ Bucket: {bucket}")
        logger.info(f"ğŸ“„ Key: {key}")

        # PDF íŒŒì¼ë§Œ ì²˜ë¦¬
        if not key.lower().endswith('.pdf'):
            logger.info("â­ï¸  Not a PDF file, skipping")
            return {
                'statusCode': 200,
                'body': json.dumps('Skipped: Not a PDF file')
            }

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_metadata_from_s3_path(key)
        logger.info(f"ğŸ·ï¸  Metadata: {metadata}")

        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(bucket, key)

        if not text or len(text) < 50:
            logger.warning("âš ï¸  í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return {
                'statusCode': 200,
                'body': json.dumps('Skipped: Text too short')
            }

        # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = chunk_text(text)

        if not chunks:
            logger.warning("âš ï¸  ìœ íš¨í•œ ì²­í¬ê°€ ì—†ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return {
                'statusCode': 200,
                'body': json.dumps('Skipped: No valid chunks')
            }

        # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        save_to_database(bucket, key, text, chunks, metadata)

        logger.info("=" * 60)
        logger.info("âœ… PDF ë²¡í„°í™” ì™„ë£Œ")
        logger.info("=" * 60)

        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'PDF processed successfully',
                'bucket': bucket,
                'key': key,
                'text_length': len(text),
                'chunk_count': len(chunks),
                'metadata': metadata
            })
        }

    except Exception as e:
        logger.error(f"âŒ Error: {e}", exc_info=True)
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }
