import boto3
import streamlit as st
from langchain_aws import ChatBedrock
# BedrockEmbeddings import ìˆ˜ì •
try:
    from langchain_aws import BedrockEmbeddings
except ImportError:
    try:
        from langchain_community.embeddings import BedrockEmbeddings
    except ImportError:
        # BedrockEmbeddingsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
        BedrockEmbeddings = None
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import PGVector
import feedparser
import tempfile
import os
import pandas as pd
import numpy as np
from datetime import datetime
import psycopg2
from sqlalchemy import create_engine, text
import json
from typing import List, Dict
import re

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í•™ì‚¬ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# AWS ë° DB ì„¤ì • (í•˜ë“œì½”ë”©)
AWS_REGION = "us-west-1"
S3_BUCKET_NAME = "ysu-ml-a-13-s3"
DATABASE_URL = "postgresql://postgres:12345678aA@a-13-rds.cpyomug2w3oq.us-west-1.rds.amazonaws.com:5432/postgres"
DB_HOST = "a-13-rds.cpyomug2w3oq.us-west-1.rds.amazonaws.com"
DB_PORT = 5432
DB_NAME = "postgres"
DB_USER = "postgres"
DB_PASSWORD = "12345678aA"

# ë©”ì¸ íƒ€ì´í‹€
st.title("ğŸ” í•™ì‚¬ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
st.caption("RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ë° ì¼ë°˜ AI ì±—ë´‡")

# í•™êµ ì„ íƒ UI (ì „ì²´ ì•±ì— ì ìš©)
def render_school_selector(engine):
    """í•™êµ ì„ íƒ UIë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    schools = get_schools_list(engine)
    
    # ì„¸ì…˜ ìƒíƒœì— ì„ íƒëœ í•™êµ ì €ì¥
    if 'selected_school' not in st.session_state:
        st.session_state.selected_school = list(schools.keys())[0]
    
    # í•™êµ ì„ íƒ ë“œë¡­ë‹¤ìš´
    selected_school = st.selectbox(
        "ğŸ« í•™êµ ì„ íƒ",
        options=list(schools.keys()),
        index=list(schools.keys()).index(st.session_state.selected_school),
        key="school_selector"
    )
    
    # ì„ íƒì´ ë³€ê²½ë˜ë©´ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
    if selected_school != st.session_state.selected_school:
        st.session_state.selected_school = selected_school
        # RSS URL ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
        st.session_state.rss_url_input = ""
        st.rerun()
    
    school_id = schools[selected_school]
    
    # ì„ íƒëœ í•™êµ ì •ë³´ í‘œì‹œ
    st.info(f"ğŸ“š í˜„ì¬ ì„ íƒ: **{selected_school}** (ID: {school_id})")
    
    return school_id, selected_school

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (EC2 IAM ì—­í•  ì‚¬ìš©)
@st.cache_resource
def init_aws_clients():
    """EC2 IAM ì—­í• ì„ ì‚¬ìš©í•˜ì—¬ AWS í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        # bedrock-runtime í´ë¼ì´ì–¸íŠ¸ë¡œ ë³€ê²½
        bedrock_client = boto3.client("bedrock-runtime", region_name=AWS_REGION)
        s3_client = boto3.client("s3", region_name=AWS_REGION)
        
        # ì„ë² ë”© ì´ˆê¸°í™” (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
        embeddings = None
        if BedrockEmbeddings is not None:
            try:
                embeddings = BedrockEmbeddings(
                    client=boto3.client("bedrock-runtime", region_name=AWS_REGION),
                    region_name=AWS_REGION,
                    model_id="cohere.embed-v4:0"
                )
            except Exception as e:
                st.warning(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        else:
            st.warning("BedrockEmbeddingsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê²€ìƒ‰ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        
        return bedrock_client, embeddings, s3_client
    except Exception as e:
        st.error(f"AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None, None

# PostgreSQL ì—°ê²° ë° ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
@st.cache_resource
def init_postgresql_vectorstore():
    """PostgreSQLì„ ë²¡í„° ìŠ¤í† ì–´ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        engine = create_engine(DATABASE_URL)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸ ë° í•„ìš”í•œ ì»¬ëŸ¼ ì¶”ê°€
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
            
            # documents í…Œì´ë¸”ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (ì—†ìœ¼ë©´)
            try:
                conn.execute(text("ALTER TABLE documents ADD COLUMN IF NOT EXISTS processed BOOLEAN DEFAULT FALSE"))
                conn.execute(text("ALTER TABLE documents ADD COLUMN IF NOT EXISTS chunks_count INTEGER DEFAULT 0"))
                conn.commit()
            except Exception as e:
                # ì»¬ëŸ¼ì´ ì´ë¯¸ ìˆê±°ë‚˜ ê¶Œí•œ ë¬¸ì œë©´ ë¬´ì‹œ
                pass
        
        return engine
    except Exception as e:
        st.error(f"PostgreSQL ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

# PGVector ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
@st.cache_resource 
def init_pgvector(_embeddings, _engine):
    """PGVector ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if not _embeddings:
        st.warning("ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë²¡í„° ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        vectorstore = PGVector(
            connection_string=DATABASE_URL,
            embedding_function=_embeddings,
            collection_name="university_docs"
        )
        return vectorstore
    except Exception as e:
        st.error(f"PGVector ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        st.warning("ë²¡í„° ê²€ìƒ‰ ëŒ€ì‹  í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None

def get_schools_list(engine):
    """í•™êµ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("SELECT id, name, code FROM schools ORDER BY name"))
            schools = result.fetchall()
            return {school[1]: school[0] for school in schools}  # {name: id} í˜•íƒœ
    except Exception as e:
        st.error(f"í•™êµ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"ì—°ì„±ëŒ€í•™êµ": 1, "ì—°ì„¸ëŒ€í•™êµ": 2}  # ê¸°ë³¸ê°’

def get_school_stats(engine, school_id):
    """ì„ íƒí•œ í•™êµì˜ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    COUNT(d.id) as total_documents,
                    SUM(CASE WHEN d.processed = true THEN 1 ELSE 0 END) as processed_documents,
                    SUM(COALESCE(d.chunks_count, 0)) as total_chunks
                FROM documents d
                WHERE d.school_id = :school_id
            """), {"school_id": school_id})
            stats = result.fetchone()
            return {
                "total_documents": stats[0] or 0,
                "processed_documents": stats[1] or 0,
                "total_chunks": stats[2] or 0
            }
    except Exception as e:
        st.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"total_documents": 0, "processed_documents": 0, "total_chunks": 0}

def upload_to_s3(file, s3_client, bucket_name, key):
    """íŒŒì¼ì„ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        s3_client.upload_fileobj(file, bucket_name, key)
        return True
    except Exception as e:
        st.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def process_pdf_from_s3(s3_client, bucket_name, key, vectorstore, embeddings, engine):
    """S3ì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ PostgreSQL DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            s3_client.download_fileobj(bucket_name, key, tmp_file)
            tmp_path = tmp_file.name
        
        # PDF ë¡œë“œ ë° ì²­í¬ ë¶„í• 
        pdf_loader = PyPDFLoader(tmp_path)
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=800,
            chunk_overlap=100,
        )
        documents = pdf_loader.load_and_split(text_splitter=splitter)
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ DBì— ì €ì¥
        chunks_processed = 0
        document_id = None
        
        with engine.connect() as conn:
            # 1. ê¸°ì¡´ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (source_url ê¸°ì¤€)
            source_url = f"s3://{bucket_name}/{key}"
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url OR file_name = :file_name
            """), {
                "source_url": source_url,
                "file_name": key.split('/')[-1]
            }).fetchone()
            
            if existing_doc:
                # ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                document_id = existing_doc[0]
                conn.execute(text("""
                    UPDATE documents 
                    SET processed = TRUE, chunks_count = :chunks_count
                    WHERE id = :document_id
                """), {
                    "document_id": document_id,
                    "chunks_count": len(documents)
                })
            else:
                # ìƒˆ ë¬¸ì„œ ìƒì„±
                result = conn.execute(text("""
                    INSERT INTO documents (school_id, file_name, source_url, category, processed, chunks_count)
                    VALUES (1, :file_name, :source_url, 'pdf', TRUE, :chunks_count)
                    RETURNING id
                """), {
                    "file_name": key.split('/')[-1],
                    "source_url": source_url,
                    "chunks_count": len(documents)
                })
                document_id = result.fetchone()[0]
            
            # 2. ê¸°ì¡´ ì²­í¬ ì‚­ì œ (ì¬ì²˜ë¦¬ì¸ ê²½ìš°)
            conn.execute(text("DELETE FROM document_chunks WHERE document_id = :document_id"), 
                         {"document_id": document_id})
            
            # 3. ìƒˆë¡œìš´ ì²­í¬ë“¤ ì €ì¥
            for i, doc in enumerate(documents):
                try:
                    # ì„ë² ë”©ì´ ìˆìœ¼ë©´ ë²¡í„°ì™€ í•¨ê»˜ ì €ì¥
                    embedding_vector = [0.0] * 1536  # ê¸°ë³¸ê°’
                    if embeddings:
                        try:
                            embedding_vector = embeddings.embed_query(doc.page_content)
                        except Exception as e:
                            st.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
                    
                    conn.execute(text("""
                        INSERT INTO document_chunks (document_id, chunk_text, embedding)
                        VALUES (:document_id, :chunk_text, :embedding)
                    """), {
                        "document_id": document_id,
                        "chunk_text": doc.page_content,
                        "embedding": embedding_vector
                    })
                    chunks_processed += 1
                except Exception as e:
                    st.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            conn.commit()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_path)
        
        return chunks_processed
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0

def process_rss_feed(rss_url, vectorstore, engine, embeddings=None):
    """RSS í”¼ë“œë¥¼ ì¤‘ë³µ ë°©ì§€í•˜ë©° DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        feed = feedparser.parse(rss_url)
        chunks_processed = 0
        skipped_duplicates = 0
        
        with engine.connect() as conn:
            # 1. ê¸°ì¡´ RSS ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url AND category = 'rss'
            """), {"source_url": rss_url}).fetchone()
            
            if existing_doc:
                document_id = existing_doc[0]
            else:
                # ìƒˆ RSS ë¬¸ì„œ ìƒì„±
                result = conn.execute(text("""
                    INSERT INTO documents (school_id, source_url, category, processed, chunks_count)
                    VALUES (1, :source_url, 'rss', FALSE, 0)
                    RETURNING id
                """), {"source_url": rss_url})
                document_id = result.fetchone()[0]
            
            # 2. ê¸°ì¡´ ì²­í¬ë“¤ì˜ ì œëª©ê³¼ ë§í¬ ì¡°íšŒ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_contents = conn.execute(text("""
                SELECT chunk_text FROM document_chunks 
                WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchall()
            
            # ê¸°ì¡´ ì œëª©ë“¤ ì¶”ì¶œ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_titles = set()
            existing_links = set()
            for content_row in existing_contents:
                content = content_row[0]
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                for line in content.split('\n'):
                    if line.strip().startswith('ì œëª©:'):
                        title = line.replace('ì œëª©:', '').strip()
                        existing_titles.add(title)
                    elif line.strip().startswith('ë§í¬:'):
                        link = line.replace('ë§í¬:', '').strip()
                        existing_links.add(link)
            
            # 3. RSS í•­ëª©ë“¤ ì²˜ë¦¬ (ì¤‘ë³µ í™•ì¸)
            for entry in feed.entries:
                entry_title = entry.get('title', '').strip()
                entry_link = entry.get('link', '').strip()
                
                # ì¤‘ë³µ í™•ì¸: ì œëª© ë˜ëŠ” ë§í¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
                if entry_title in existing_titles or entry_link in existing_links:
                    skipped_duplicates += 1
                    continue
                
                # ìƒˆë¡œìš´ í•­ëª© ì²˜ë¦¬
                content = f"""
ì œëª©: {entry_title}
ë‚´ìš©: {entry.get('summary', entry.get('description', ''))}
ë§í¬: {entry_link}
ë°œí–‰ì¼: {entry.get('published', '')}
                """
                
                # í…ìŠ¤íŠ¸ ë¶„í• 
                splitter = CharacterTextSplitter.from_tiktoken_encoder(
                    separator="\n",
                    chunk_size=800,
                    chunk_overlap=100,
                )
                chunks = splitter.split_text(content)
                
                for chunk in chunks:
                    try:
                        # ì„ë² ë”©ì´ ìˆìœ¼ë©´ ë²¡í„°ì™€ í•¨ê»˜ ì €ì¥
                        embedding_vector = [0.0] * 1536  # ê¸°ë³¸ê°’
                        if embeddings:
                            try:
                                embedding_vector = embeddings.embed_query(chunk)
                            except:
                                pass
                        
                        conn.execute(text("""
                            INSERT INTO document_chunks (document_id, chunk_text, embedding)
                            VALUES (:document_id, :chunk_text, :embedding)
                        """), {
                            "document_id": document_id,
                            "chunk_text": chunk,
                            "embedding": embedding_vector
                        })
                        chunks_processed += 1
                        
                        # ìƒˆë¡œ ì¶”ê°€ëœ ì œëª©ê³¼ ë§í¬ë¥¼ ê¸°ì¡´ ì„¸íŠ¸ì— ì¶”ê°€ (ë‹¤ìŒ í•­ëª© ì¤‘ë³µ í™•ì¸ìš©)
                        existing_titles.add(entry_title)
                        existing_links.add(entry_link)
                        
                    except Exception as e:
                        st.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # 4. ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì´ ì²­í¬ ìˆ˜ ê³„ì‚°)
            total_chunks = conn.execute(text("""
                SELECT COUNT(*) FROM document_chunks WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchone()[0]
            
            conn.execute(text("""
                UPDATE documents 
                SET processed = TRUE, chunks_count = :chunks_count
                WHERE id = :document_id
            """), {
                "document_id": document_id,
                "chunks_count": total_chunks
            })
            
            conn.commit()
        
        # RSS í”¼ë“œ ì •ë³´ë¥¼ rss_feeds í…Œì´ë¸”ì—ë„ ì €ì¥ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
        try:
            with engine.connect() as conn:
                conn.execute(text("""
                    INSERT INTO rss_feeds (school_id, url)
                    VALUES (1, :rss_url)
                    ON CONFLICT (url) DO NOTHING
                """), {"rss_url": rss_url})
                conn.commit()
        except:
            pass  # rss_feeds í…Œì´ë¸” ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
        
        # ê²°ê³¼ ë©”ì‹œì§€ì— ì¤‘ë³µ ìŠ¤í‚µ ì •ë³´ í¬í•¨
        if skipped_duplicates > 0:
            st.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì‹ ê·œ {chunks_processed}ê°œ ì²­í¬ ì¶”ê°€, ì¤‘ë³µ {skipped_duplicates}ê°œ í•­ëª© ìŠ¤í‚µ")
        
        return chunks_processed
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0

def get_school_code_by_id(engine, school_id):
    """school_idë¡œ í•™êµ ì½”ë“œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("SELECT code FROM schools WHERE id = :school_id"), 
                                 {"school_id": school_id})
            school = result.fetchone()
            return school[0] if school else "UNK"
    except Exception as e:
        return "UNK"

def save_file_metadata(engine, filename, s3_key, doc_type, school_id=1):
    """íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ documents í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤ (school_id í¬í•¨)."""
    try:
        with engine.connect() as conn:
            # source_urlë¡œ ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
            source_url = s3_key if s3_key.startswith('s3://') else f"s3://{S3_BUCKET_NAME}/{s3_key}"
            
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url OR file_name = :filename
            """), {"source_url": source_url, "filename": filename}).fetchone()
            
            if not existing_doc:
                # ìƒˆ ë¬¸ì„œë§Œ ì €ì¥ (ì¤‘ë³µ ë°©ì§€) - school_id í¬í•¨
                conn.execute(text("""
                    INSERT INTO documents (school_id, file_name, source_url, category, processed, chunks_count)
                    VALUES (:school_id, :filename, :source_url, :doc_type, FALSE, 0)
                """), {
                    "school_id": school_id,
                    "filename": filename,
                    "source_url": source_url,
                    "doc_type": doc_type
                })
                conn.commit()
        return True
    except Exception as e:
        st.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def get_file_metadata(engine, school_id=None):
    """documents í…Œì´ë¸”ì—ì„œ íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (í•™êµë³„ í•„í„°ë§)."""
    try:
        if school_id:
            # íŠ¹ì • í•™êµì˜ íŒŒì¼ë§Œ ì¡°íšŒ
            df = pd.read_sql("""
                SELECT d.id, d.file_name as filename, 
                       d.source_url as s3_key, 
                       d.created_at as upload_date, 
                       d.category as document_type, 
                       COALESCE(d.processed, FALSE) as processed, 
                       COALESCE(d.chunks_count, 0) as chunks_count
                FROM documents d
                WHERE d.category != 'rss' AND d.school_id = %(school_id)s
                ORDER BY d.created_at DESC
            """, engine, params={"school_id": school_id})
        else:
            # ëª¨ë“  í•™êµì˜ íŒŒì¼ ì¡°íšŒ
            df = pd.read_sql("""
                SELECT id, file_name as filename, 
                       source_url as s3_key, 
                       created_at as upload_date, 
                       category as document_type, 
                       COALESCE(processed, FALSE) as processed, 
                       COALESCE(chunks_count, 0) as chunks_count
                FROM documents 
                WHERE category != 'rss'
                ORDER BY created_at DESC
            """, engine)
        return df
    except Exception as e:
        st.error(f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def get_rss_feeds(engine, school_id=None):
    """RSS í”¼ë“œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤ (rss_feeds í…Œì´ë¸” ê¸°ë°˜)."""
    try:
        if school_id:
            # íŠ¹ì • í•™êµì˜ RSSë§Œ ì¡°íšŒ
            df = pd.read_sql("""
                SELECT rf.id, rf.url as rss_url, rf.title, rf.last_processed,
                        rf.processed_count, rf.status, rf.created_at
                FROM rss_feeds rf
                WHERE rf.school_id = %(school_id)s
                ORDER BY rf.created_at DESC
            """, engine, params={"school_id": school_id})
        else:
            # ëª¨ë“  í•™êµì˜ RSS ì¡°íšŒ
            df = pd.read_sql("""
                SELECT rf.id, rf.url as rss_url, rf.title, rf.last_processed,
                        rf.processed_count, rf.status, rf.created_at,
                        s.name as school_name
                FROM rss_feeds rf
                JOIN schools s ON rf.school_id = s.id
                ORDER BY rf.created_at DESC
            """, engine)
        return df
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def add_rss_feed(engine, school_id, rss_url):
    """ìƒˆ RSS í”¼ë“œë¥¼ rss_feeds í…Œì´ë¸”ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        # RSS í”¼ë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        feed = feedparser.parse(rss_url)
        feed_title = feed.feed.get('title', rss_url)
        
        with engine.connect() as conn:
            # ì¤‘ë³µ í™•ì¸ ë° ì¶”ê°€
            result = conn.execute(text("""
                INSERT INTO rss_feeds (school_id, url, title, status)
                VALUES (:school_id, :url, :title, 'active')
                ON CONFLICT (school_id, url) DO NOTHING
                RETURNING id
            """), {
                "school_id": school_id,
                "url": rss_url,
                "title": feed_title
            })
            
            new_feed = result.fetchone()
            conn.commit()
            
            if new_feed:
                return new_feed[0]  # ìƒˆë¡œ ìƒì„±ëœ ID
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ê¸°ì¡´ ID ë°˜í™˜
                existing = conn.execute(text("""
                    SELECT id FROM rss_feeds 
                    WHERE school_id = :school_id AND url = :url
                """), {"school_id": school_id, "url": rss_url}).fetchone()
                return existing[0] if existing else None
                
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
        return None

def delete_rss_feed(engine, rss_feed_id):
    """RSS í”¼ë“œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤ (ê´€ë ¨ documentsì™€ chunksë„ í•¨ê»˜)."""
    try:
        with engine.connect() as conn:
            # RSS í”¼ë“œ URL ì¡°íšŒ
            rss_info = conn.execute(text("""
                SELECT url, school_id FROM rss_feeds WHERE id = :rss_id
            """), {"rss_id": rss_feed_id}).fetchone()
            
            if not rss_info:
                return False
            
            rss_url, school_id = rss_info
            
            # ê´€ë ¨ documents ì°¾ê¸°
            docs = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :url AND category = 'rss' AND school_id = :school_id
            """), {"url": rss_url, "school_id": school_id}).fetchall()
            
            # documentsì™€ ì—°ê´€ëœ chunks ì‚­ì œ
            for doc in docs:
                conn.execute(text("""
                    DELETE FROM document_chunks WHERE document_id = :doc_id
                """), {"doc_id": doc[0]})
            
            # documents ì‚­ì œ
            conn.execute(text("""
                DELETE FROM documents 
                WHERE source_url = :url AND category = 'rss' AND school_id = :school_id
            """), {"url": rss_url, "school_id": school_id})
            
            # rss_feeds ì‚­ì œ
            conn.execute(text("""
                DELETE FROM rss_feeds WHERE id = :rss_id
            """), {"rss_id": rss_feed_id})
            
            conn.commit()
            return True
            
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False

def search_documents(query, vectorstore, engine, school_id=None):
    """ê°œì„ ëœ ë¬¸ì„œ ê²€ìƒ‰: í•™êµë³„ í•„í„°ë§ + ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    try:
        results = []
        
        with engine.connect() as conn:
            # í•™êµë³„ í•„í„°ë§ ì¿¼ë¦¬
            school_filter = "AND d.school_id = :school_id" if school_id else ""
            params = {"query": f"%{query}%"}
            if school_id:
                params["school_id"] = school_id
            
            # 1. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í•™êµë³„ í•„í„°ë§)
            keyword_results = conn.execute(text(f"""
                SELECT dc.chunk_text, d.source_url, d.file_name, d.category, d.created_at,
                        ROW_NUMBER() OVER (ORDER BY d.created_at DESC) as rank
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE dc.chunk_text ILIKE :query {school_filter}
                ORDER BY d.created_at DESC
                LIMIT 10
            """), params).fetchall()
            
            # 2. ë‹¨ì–´ë³„ ë¶„í•´ ê²€ìƒ‰ (ìœ ì‚¬ë„ í–¥ìƒ)
            query_words = [word.strip() for word in query.split() if len(word.strip()) > 1]
            if len(query_words) > 1:
                word_conditions = " OR ".join([f"dc.chunk_text ILIKE :word_{i}" for i in range(len(query_words))])
                word_params = {f"word_{i}": f"%{word}%" for i, word in enumerate(query_words)}
                word_params.update({"original_query": f"%{query}%"})
                if school_id:
                    word_params["school_id"] = school_id
                
                similarity_results = conn.execute(text(f"""
                    SELECT dc.chunk_text, d.source_url, d.file_name, d.category, d.created_at,
                           ROW_NUMBER() OVER (ORDER BY d.created_at DESC) as rank
                    FROM document_chunks dc
                    JOIN documents d ON dc.document_id = d.id
                    WHERE ({word_conditions})
                    AND dc.chunk_text NOT ILIKE :original_query {school_filter}
                    ORDER BY d.created_at DESC
                    LIMIT 5
                """), word_params).fetchall()
            else:
                similarity_results = []
            
            # 3. ê²°ê³¼ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°
            all_results = list(keyword_results) + list(similarity_results)
            seen_texts = set()
            unique_results = []
            
            for row in all_results:
                text_preview = row.chunk_text[:100]
                if text_preview not in seen_texts:
                    seen_texts.add(text_preview)
                    unique_results.append(row)
                
                if len(unique_results) >= 8:
                    break
            
            # 4. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° Document í˜•íƒœë¡œ ë³€í™˜
            from langchain.schema import Document
            scored_results = []
            
            for i, row in enumerate(unique_results):
                title = extract_title_from_text(row.chunk_text)
                
                source_info = "RSS ê³µì§€ì‚¬í•­"
                if row.category == 'pdf':
                    source_info = row.file_name or "PDF ë¬¸ì„œ"
                elif row.category == 'rss':
                    source_info = "RSS ê³µì§€ì‚¬í•­"
                
                metadata = {
                    "source": row.source_url or "unknown",
                    "filename": source_info, 
                    "category": row.category or "unknown",
                    "date": row.created_at.strftime("%Y-%m-%d") if row.created_at else "N/A",
                    "title": title,
                    "rank": i + 1
                }
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = calculate_relevance_score(query, row.chunk_text, metadata)
                metadata["relevance_score"] = relevance_score
                
                scored_results.append({
                    'document': Document(page_content=row.chunk_text, metadata=metadata),
                    'score': relevance_score
                })
            
            # 5. ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ ë° í•„í„°ë§
            scored_results.sort(key=lambda x: x['score'], reverse=True)
            
            # 6. ë†’ì€ ê´€ë ¨ì„± ê²°ê³¼ë§Œ ë°˜í™˜ (ì„ê³„ê°’: 0.2ë¡œ í•˜í–¥ ì¡°ì •)
            high_relevance_results = []
            for item in scored_results:
                if item['score'] >= 0.2:  # ì„ê³„ê°’ ëŒ€í­ í•˜í–¥ ì¡°ì •
                    high_relevance_results.append(item['document'])
                if len(high_relevance_results) >= 5:  # ìµœëŒ€ 5ê°œ
                    break
        
        return high_relevance_results
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return []

def process_rss_feed(rss_url, vectorstore, engine, embeddings=None, school_id=1):
    """RSS í”¼ë“œë¥¼ ì²˜ë¦¬í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤ (rss_feeds í…Œì´ë¸” ê´€ë¦¬ í¬í•¨)."""
    try:
        feed = feedparser.parse(rss_url)
        chunks_processed = 0
        skipped_duplicates = 0
        
        with engine.connect() as conn:
            # 1. RSS í”¼ë“œë¥¼ rss_feeds í…Œì´ë¸”ì— ë“±ë¡ (ì—†ìœ¼ë©´ ì¶”ê°€)
            feed_title = feed.feed.get('title', rss_url)
            
            rss_feed_result = conn.execute(text("""
                INSERT INTO rss_feeds (school_id, url, title, status)
                VALUES (:school_id, :url, :title, 'active')
                ON CONFLICT (school_id, url) DO UPDATE SET
                    title = EXCLUDED.title,
                    last_processed = NOW()
                RETURNING id
            """), {
                "school_id": school_id,
                "url": rss_url,
                "title": feed_title
            }).fetchone()
            
            rss_feed_id = rss_feed_result[0]
            
            # 2. documents í…Œì´ë¸”ì—ì„œ RSS ë¬¸ì„œ ì°¾ê¸°/ìƒì„±
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url AND category = 'rss' AND school_id = :school_id
            """), {"source_url": rss_url, "school_id": school_id}).fetchone()
            
            if existing_doc:
                document_id = existing_doc[0]
            else:
                # ìƒˆ RSS ë¬¸ì„œ ìƒì„±
                result = conn.execute(text("""
                    INSERT INTO documents (school_id, source_url, category, processed, chunks_count)
                    VALUES (:school_id, :source_url, 'rss', FALSE, 0)
                    RETURNING id
                """), {"school_id": school_id, "source_url": rss_url})
                document_id = result.fetchone()[0]
            
            # 3. ê¸°ì¡´ ì²­í¬ë“¤ì˜ ì œëª©ê³¼ ë§í¬ ì¡°íšŒ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_contents = conn.execute(text("""
                SELECT chunk_text FROM document_chunks 
                WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchall()
            
            # ê¸°ì¡´ ì œëª©ë“¤ ì¶”ì¶œ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_titles = set()
            existing_links = set()
            for content_row in existing_contents:
                content = content_row[0]
                for line in content.split('\n'):
                    if line.strip().startswith('ì œëª©:'):
                        title = line.replace('ì œëª©:', '').strip()
                        existing_titles.add(title)
                    elif line.strip().startswith('ë§í¬:'):
                        link = line.replace('ë§í¬:', '').strip()
                        existing_links.add(link)
            
            # 4. RSS í•­ëª©ë“¤ ì²˜ë¦¬ (ì¤‘ë³µ í™•ì¸)
            for entry in feed.entries:
                entry_title = entry.get('title', '').strip()
                entry_link = entry.get('link', '').strip()
                
                # ì¤‘ë³µ í™•ì¸: ì œëª© ë˜ëŠ” ë§í¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
                if entry_title in existing_titles or entry_link in existing_links:
                    skipped_duplicates += 1
                    continue
                
                # ìƒˆë¡œìš´ í•­ëª© ì²˜ë¦¬
                content = f"""
ì œëª©: {entry_title}
ë‚´ìš©: {entry.get('summary', entry.get('description', ''))}
ë§í¬: {entry_link}
ë°œí–‰ì¼: {entry.get('published', '')}
                """
                
                splitter = CharacterTextSplitter.from_tiktoken_encoder(
                    separator="\n",
                    chunk_size=800,
                    chunk_overlap=100,
                )
                chunks = splitter.split_text(content)
                
                for chunk in chunks:
                    try:
                        embedding_vector = [0.0] * 1536  # ê¸°ë³¸ê°’
                        if embeddings:
                            try:
                                embedding_vector = embeddings.embed_query(chunk)
                            except:
                                pass
                        
                        conn.execute(text("""
                            INSERT INTO document_chunks (document_id, chunk_text, embedding)
                            VALUES (:document_id, :chunk_text, :embedding)
                        """), {
                            "document_id": document_id,
                            "chunk_text": chunk,
                            "embedding": embedding_vector
                        })
                        chunks_processed += 1
                        
                        existing_titles.add(entry_title)
                        existing_links.add(entry_link)
                        
                    except Exception as e:
                        st.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # 5. documents í…Œì´ë¸” ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            total_chunks = conn.execute(text("""
                SELECT COUNT(*) FROM document_chunks WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchone()[0]
            
            conn.execute(text("""
                UPDATE documents 
                SET processed = TRUE, chunks_count = :chunks_count
                WHERE id = :document_id
            """), {
                "document_id": document_id,
                "chunks_count": total_chunks
            })
            
            # 6. rss_feeds í…Œì´ë¸” ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            conn.execute(text("""
                UPDATE rss_feeds 
                SET last_processed = NOW(), processed_count = :processed_count
                WHERE id = :rss_feed_id
            """), {
                "rss_feed_id": rss_feed_id,
                "processed_count": total_chunks
            })
            
            conn.commit()
        
        if skipped_duplicates > 0:
            st.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì‹ ê·œ {chunks_processed}ê°œ ì²­í¬ ì¶”ê°€, ì¤‘ë³µ {skipped_duplicates}ê°œ í•­ëª© ìŠ¤í‚µ")
        
        return chunks_processed
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0
    """RSS í”¼ë“œë¥¼ ì²˜ë¦¬í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤ (school_id í¬í•¨)."""
    try:
        feed = feedparser.parse(rss_url)
        chunks_processed = 0
        skipped_duplicates = 0
        
        with engine.connect() as conn:
            # 1. ê¸°ì¡´ RSS ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url AND category = 'rss' AND school_id = :school_id
            """), {"source_url": rss_url, "school_id": school_id}).fetchone()
            
            if existing_doc:
                document_id = existing_doc[0]
            else:
                # ìƒˆ RSS ë¬¸ì„œ ìƒì„±
                result = conn.execute(text("""
                    INSERT INTO documents (school_id, source_url, category, processed, chunks_count)
                    VALUES (:school_id, :source_url, 'rss', FALSE, 0)
                    RETURNING id
                """), {"school_id": school_id, "source_url": rss_url})
                document_id = result.fetchone()[0]
            
            # ê¸°ì¡´ ì²­í¬ë“¤ì˜ ì œëª©ê³¼ ë§í¬ ì¡°íšŒ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_contents = conn.execute(text("""
                SELECT chunk_text FROM document_chunks 
                WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchall()
            
            # ê¸°ì¡´ ì œëª©ë“¤ ì¶”ì¶œ (ì¤‘ë³µ í™•ì¸ìš©)
            existing_titles = set()
            existing_links = set()
            for content_row in existing_contents:
                content = content_row[0]
                for line in content.split('\n'):
                    if line.strip().startswith('ì œëª©:'):
                        title = line.replace('ì œëª©:', '').strip()
                        existing_titles.add(title)
                    elif line.strip().startswith('ë§í¬:'):
                        link = line.replace('ë§í¬:', '').strip()
                        existing_links.add(link)
            
            # RSS í•­ëª©ë“¤ ì²˜ë¦¬ (ì¤‘ë³µ í™•ì¸)
            for entry in feed.entries:
                entry_title = entry.get('title', '').strip()
                entry_link = entry.get('link', '').strip()
                
                # ì¤‘ë³µ í™•ì¸: ì œëª© ë˜ëŠ” ë§í¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
                if entry_title in existing_titles or entry_link in existing_links:
                    skipped_duplicates += 1
                    continue
                
                # ìƒˆë¡œìš´ í•­ëª© ì²˜ë¦¬
                content = f"""
ì œëª©: {entry_title}
ë‚´ìš©: {entry.get('summary', entry.get('description', ''))}
ë§í¬: {entry_link}
ë°œí–‰ì¼: {entry.get('published', '')}
                """
                
                splitter = CharacterTextSplitter.from_tiktoken_encoder(
                    separator="\n",
                    chunk_size=800,
                    chunk_overlap=100,
                )
                chunks = splitter.split_text(content)
                
                for chunk in chunks:
                    try:
                        embedding_vector = [0.0] * 1536  # ê¸°ë³¸ê°’
                        if embeddings:
                            try:
                                embedding_vector = embeddings.embed_query(chunk)
                            except:
                                pass
                        
                        conn.execute(text("""
                            INSERT INTO document_chunks (document_id, chunk_text, embedding)
                            VALUES (:document_id, :chunk_text, :embedding)
                        """), {
                            "document_id": document_id,
                            "chunk_text": chunk,
                            "embedding": embedding_vector
                        })
                        chunks_processed += 1
                        
                        existing_titles.add(entry_title)
                        existing_links.add(entry_link)
                        
                    except Exception as e:
                        st.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            total_chunks = conn.execute(text("""
                SELECT COUNT(*) FROM document_chunks WHERE document_id = :document_id
            """), {"document_id": document_id}).fetchone()[0]
            
            conn.execute(text("""
                UPDATE documents 
                SET processed = TRUE, chunks_count = :chunks_count
                WHERE id = :document_id
            """), {
                "document_id": document_id,
                "chunks_count": total_chunks
            })
            
            conn.commit()
        
        if skipped_duplicates > 0:
            st.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì‹ ê·œ {chunks_processed}ê°œ ì²­í¬ ì¶”ê°€, ì¤‘ë³µ {skipped_duplicates}ê°œ í•­ëª© ìŠ¤í‚µ")
        
        return chunks_processed
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0

def delete_document_from_db(document_id, engine):
    """íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ documents í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with engine.connect() as conn:
            # source_urlë¡œ ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
            source_url = s3_key if s3_key.startswith('s3://') else f"s3://{S3_BUCKET_NAME}/{s3_key}"
            
            existing_doc = conn.execute(text("""
                SELECT id FROM documents 
                WHERE source_url = :source_url OR file_name = :filename
            """), {"source_url": source_url, "filename": filename}).fetchone()
            
            if not existing_doc:
                # ìƒˆ ë¬¸ì„œë§Œ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
                conn.execute(text("""
                    INSERT INTO documents (school_id, file_name, source_url, category, processed, chunks_count)
                    VALUES (1, :filename, :source_url, :doc_type, FALSE, 0)
                """), {
                    "filename": filename,
                    "source_url": source_url,
                    "doc_type": doc_type
                })
                conn.commit()
        return True
    except Exception as e:
        st.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def delete_document_from_db(document_id, engine):
    """ë¬¸ì„œë¥¼ DBì—ì„œ ì™„ì „íˆ ì‚­ì œí•©ë‹ˆë‹¤ (document_chunks + documents)."""
    try:
        with engine.connect() as conn:
            # 1. ë¨¼ì € document_chunksì—ì„œ ì‚­ì œ (Foreign Key ë•Œë¬¸ì—)
            chunks_result = conn.execute(text("""
                DELETE FROM document_chunks 
                WHERE document_id = :document_id
            """), {"document_id": document_id})
            
            # 2. documents í…Œì´ë¸”ì—ì„œ ì‚­ì œ
            docs_result = conn.execute(text("""
                DELETE FROM documents 
                WHERE id = :document_id
            """), {"document_id": document_id})
            
            conn.commit()
            
            return chunks_result.rowcount, docs_result.rowcount
    except Exception as e:
        st.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return 0, 0

def extract_title_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if line and 'ì œëª©:' in line:
            return line.replace('ì œëª©:', '').strip()
        elif line and len(line) > 10 and len(line) < 100:
            # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„ì„ ì œëª©ìœ¼ë¡œ ê°„ì£¼
            return line
    return text[:50] + "..." if len(text) > 50 else text

# ìì—°ì–´ ì¿¼ë¦¬ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_query(query):
    """ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ì—', 'ëŒ€í•´', 'ëŒ€í•œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 
                    'ê¶ê¸ˆí•©ë‹ˆë‹¤', 'ê¶ê¸ˆí•´ìš”', 'ì•Œê³ ì‹¶ì–´ìš”', 'ì•Œë ¤ì£¼ì„¸ìš”', 'ë¬¸ì˜', 'ì§ˆë¬¸', 
                    'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ë¬´ì—‡', 'ì™œ', 'ì–´ë–¤', 'ì…ë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”',
                    'ì¤‘ì—ì„œ', 'ê´€ë ¨í•´ì„œ', 'ê´€ë ¨í•˜ì—¬', 'ì— ê´€í•´', 'ì— ê´€í•œ', 'ê²ƒ', 'ìˆ˜', 'ìˆ', 'ì—†']
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.sub(r'[^\wê°€-í£\s]', ' ', query).split()
        core_keywords = []
        
        for word in words:
            word = word.strip()
            if len(word) > 1 and word not in stopwords:
                core_keywords.append(word)
        
        # ì›ë³¸ ì¿¼ë¦¬ì™€ í•µì‹¬ í‚¤ì›Œë“œ ì¡°í•© ë°˜í™˜
        core_query = ' '.join(core_keywords)
        
        return {
            'original': query,
            'processed': core_query,
            'keywords': core_keywords
        }
    except:
        return {
            'original': query,
            'processed': query,
            'keywords': query.split()
        }

def calculate_keyword_score(query, document_content):
    """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì‚¬ìš©)"""
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        query_data = preprocess_query(query)
        original_query = query_data['original'].lower()
        processed_query = query_data['processed'].lower()
        core_keywords = set([kw.lower() for kw in query_data['keywords']])
        
        doc_text = document_content.lower()
        doc_words = set(re.sub(r'[^\wê°€-í£]', ' ', doc_text).split())
        
        if not core_keywords:
            return 0.0
            
        # 1. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        matched_keywords = core_keywords.intersection(doc_words)
        keyword_ratio = len(matched_keywords) / len(core_keywords) if core_keywords else 0
        
        # 2. êµ¬ë¬¸ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì›ë³¸ ì¿¼ë¦¬ì™€ ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ëª¨ë‘ í™•ì¸)
        phrase_bonus = 0
        if original_query.strip() in doc_text:
            phrase_bonus += 0.4
        elif processed_query.strip() in doc_text:
            phrase_bonus += 0.3
        
        # 3. í•µì‹¬ í‚¤ì›Œë“œë³„ ê°œë³„ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        individual_bonus = 0
        for keyword in core_keywords:
            if len(keyword) > 2 and keyword in doc_text:
                individual_bonus += 0.15
        
        # 4. ì—°ì† í‚¤ì›Œë“œ ë§¤ì¹­ (ì˜ˆ: "êµì›ì—°ìˆ˜" â†’ "êµì›" + "ì—°ìˆ˜")
        sequence_bonus = 0
        if len(core_keywords) >= 2:
            keyword_list = list(core_keywords)
            for i in range(len(keyword_list)-1):
                combined = keyword_list[i] + keyword_list[i+1]
                if combined in doc_text:
                    sequence_bonus += 0.2
        
        final_score = keyword_ratio + phrase_bonus + individual_bonus + sequence_bonus
        return min(final_score, 1.0)
        
    except:
        return 0.0

def calculate_category_score(query, document_content, metadata):
    """ì¹´í…Œê³ ë¦¬ ë° íŠ¹ìˆ˜ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚° (ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì‚¬ìš©)"""
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        query_data = preprocess_query(query)
        core_keywords = [kw.lower() for kw in query_data['keywords']]
        content_lower = document_content.lower()
        
        # ê¸°ë³¸ ì ìˆ˜ë¥¼ ë†’ê²Œ ì‹œì‘
        base_score = 0.8
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜ (í™•ì¥)
        category_groups = {
            'êµì›ê´€ë¦¬': {
                'keywords': ['êµì›', 'êµìˆ˜', 'ì—°ìˆ˜', 'ì´ˆë¹™', 'ì±„ìš©', 'ì¸ì‚¬', 'í•™ìˆ ', 'ì—°êµ¬'],
                'negative': []
            },
            'ì¼ë°˜_ì…í•™': {
                'keywords': ['ì…í•™', 'ì‹ ì…ìƒ', 'ëª¨ì§‘', 'ì§€ì›', 'ì „í˜•', 'ìˆ˜ì‹œ', 'ì •ì‹œ', 'ì…ì‹œìƒë‹´'],
                'negative': ['ìœ„íƒ', 'ì‚°ì—…ì²´ìœ„íƒêµìœ¡']
            },
            'íŠ¹ìˆ˜_ì…í•™': {
                'keywords': ['ìœ„íƒêµìœ¡', 'ì‚°ì—…ì²´', 'í¸ì…', 'ì „ê³µì‹¬í™”', 'ì¬ì…í•™'],
                'negative': []
            },
            'í•™ì‚¬ê´€ë¦¬': {
                'keywords': ['ìˆ˜ê°•ì‹ ì²­', 'í•™ì ', 'ì„±ì ', 'ì¡¸ì—…', 'íœ´í•™', 'ë³µí•™', 'ê·œì •'],
                'negative': []
            },
            'í•™ìƒí™œë™': {
                'keywords': ['gem-festival', 'festival', 'ì¶•ì œ', 'ë™ì•„ë¦¬', 'í–‰ì‚¬'],
                'negative': []
            },
            'í•™ìƒì§€ì›': {
                'keywords': ['ì¥í•™ê¸ˆ', 'ì·¨ì—…', 'ìƒë‹´', 'ë³µì§€'],
                'negative': []
            }
        }
        
        max_score = base_score
        
        for category, rules in category_groups.items():
            score = base_score
            matched_positive = 0
            
            # í•µì‹¬ í‚¤ì›Œë“œì™€ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤ì¹­
            for category_kw in rules['keywords']:
                for core_kw in core_keywords:
                    if category_kw in core_kw or core_kw in category_kw:
                        if category_kw in content_lower:
                            matched_positive += 1
                            score += 0.15
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´ ì¶”ê°€ ë³´ë„ˆìŠ¤
            if matched_positive >= 2:
                score += 0.1
            
            # ë¶€ì • í‚¤ì›Œë“œ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
            for neg_keyword in rules['negative']:
                if neg_keyword in content_lower and not any(neg_keyword in ck for ck in core_keywords):
                    score -= 0.25
            
            max_score = max(max_score, score)
        
        return max(min(max_score, 1.0), 0.0)
    except:
        return 0.8

def calculate_context_score(query, document_content):
    """ë¬¸ë§¥ì  ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ì¿¼ë¦¬ ê¸¸ì´ì— ê´€ê³„ì—†ì´ ì¼ê´€ì„± ìœ ì§€)"""
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        query_data = preprocess_query(query)
        core_keywords = query_data['keywords']
        
        # ê¸°ë³¸ ì ìˆ˜ (ì¿¼ë¦¬ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¼ì •í•˜ê²Œ)
        base_score = 0.85
        
        # ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
        doc_length = len(document_content.split())
        length_score = 1.0
        
        if doc_length < 5:
            length_score = 0.6
        elif doc_length > 1000:
            length_score = 0.9
            
        # ì •ë³´ ì œê³µì„± í‰ê°€
        info_indicators = ['ì•ˆë‚´', 'ê³µì§€', 'ì•Œë¦¼', 'ì¼ì •', 'ë°©ë²•', 'ì ˆì°¨', 'ì‹ ì²­', 'ê·œì •', 'ì§€ì¹¨']
        has_info_content = any(indicator in document_content for indicator in info_indicators)
        info_bonus = 0.1 if has_info_content else 0
        
        final_score = (base_score + info_bonus) * length_score
        return min(final_score, 1.0)
        
    except:
        return 0.85

# ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ë“¤
def calculate_relevance_score(query, document_content, metadata):
    """í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ìì—°ì–´ ì¿¼ë¦¬ ìµœì í™” ë²„ì „)"""
    try:
        # ê° ì ìˆ˜ ê³„ì‚°
        keyword_score = calculate_keyword_score(query, document_content)
        category_score = calculate_category_score(query, document_content, metadata)
        context_score = calculate_context_score(query, document_content)
        
        # ê°€ì¤‘ í‰ê·  (í‚¤ì›Œë“œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜, ìì—°ì–´ ì¹œí™”ì )
        final_score = (keyword_score * 0.65) + (category_score * 0.25) + (context_score * 0.10)
        
        # ìì—°ì–´ ë¬¸ì¥ì— ëŒ€í•œ ì¶”ê°€ ë³´ë„ˆìŠ¤
        query_data = preprocess_query(query)
        if len(query_data['original'].split()) > 3:  # ìì—°ì–´ ë¬¸ì¥ì¸ ê²½ìš°
            # í‚¤ì›Œë“œ ë°€ë„ê°€ ë†’ìœ¼ë©´ ë³´ë„ˆìŠ¤
            keyword_density = len(query_data['keywords']) / len(query_data['original'].split())
            if keyword_density > 0.5:
                final_score = min(final_score + 0.1, 1.0)
        
        return round(final_score, 3)
    except Exception as e:
        print(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
        return 0.5

def get_relevance_indicator(score):
    """ì ìˆ˜ì— ë”°ë¥¸ ê´€ë ¨ì„± ì§€ì‹œì ë°˜í™˜ (ì¡°ì •ëœ ì„ê³„ê°’)"""
    if score >= 0.75:
        return "âœ…", "ë†’ìŒ", "success"
    elif score >= 0.50:
        return "âš ï¸", "ë³´í†µ", "warning"
    else:
        return "âŒ", "ë‚®ìŒ", "error"

# ë¶€ì„œ ê²€ìƒ‰ fallback í•¨ìˆ˜ë“¤
def find_relevant_department(query, school_id, engine):
    """
    ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ë¶€ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ë¶€ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        with engine.connect() as conn:
            # ë¶€ì„œë³„ í‚¤ì›Œë“œì™€ ì§ì› ì •ë³´ ì¡°íšŒ
            result = conn.execute(text("""
                SELECT 
                    d.id, d.name, d.description, d.main_phone,
                    bk.keyword, bk.weight,
                    s.name as staff_name, s.position, s.phone, s.email, 
                    s.responsibilities, s.is_head
                FROM departments d 
                LEFT JOIN business_keywords bk ON d.id = bk.department_id
                LEFT JOIN staff_members s ON d.id = s.department_id AND s.is_head = TRUE
                WHERE d.school_id = :school_id
                ORDER BY d.name, bk.weight DESC
            """), {"school_id": school_id}).fetchall()
            
            if not result:
                return None
                
            # ì§ˆë¬¸ ì „ì²˜ë¦¬ (ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì œê±°)
            query_processed = re.sub(r'[^\wê°€-í£]', ' ', query.lower()).strip()
            query_words = query_processed.split()
            
            # ë¶€ì„œë³„ ì ìˆ˜ ê³„ì‚°
            department_scores = {}
            department_info = {}
            
            for row in result:
                dept_id = row[0]
                dept_name = row[1]
                keyword = row[4]
                weight = row[5] if row[5] else 1
                
                # ë¶€ì„œ ì •ë³´ ì €ì¥ (ì²˜ìŒ í•œ ë²ˆë§Œ)
                if dept_id not in department_info:
                    department_info[dept_id] = {
                        'name': dept_name,
                        'description': row[2],
                        'main_phone': row[3],
                        'staff_name': row[6],
                        'staff_position': row[7],
                        'staff_phone': row[8],
                        'staff_email': row[9],
                        'staff_responsibilities': row[10]
                    }
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                if keyword and dept_id not in department_scores:
                    department_scores[dept_id] = 0
                
                if keyword:
                    keyword_lower = keyword.lower()
                    # ì™„ì „ ë§¤ì¹­ (ë†’ì€ ì ìˆ˜)
                    if keyword_lower in query_processed:
                        department_scores[dept_id] += weight * 3
                    # ë¶€ë¶„ ë§¤ì¹­ (ì¤‘ê°„ ì ìˆ˜)
                    elif any(word in keyword_lower or keyword_lower in word for word in query_words):
                        department_scores[dept_id] += weight * 2
                    # ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­ (ë‚®ì€ ì ìˆ˜)
                    elif any(is_similar_keyword(word, keyword_lower) for word in query_words):
                        department_scores[dept_id] += weight
            
            # ì ìˆ˜ê°€ ë†’ì€ ë¶€ì„œ ë°˜í™˜
            if department_scores:
                best_dept_id = max(department_scores.items(), key=lambda x: x[1])
                if best_dept_id[1] > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                    return department_info[best_dept_id[0]]
            
        return None
    except Exception as e:
        print(f"ë¶€ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return None

def is_similar_keyword(word, keyword):
    """ìœ ì‚¬í•œ í‚¤ì›Œë“œì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    similar_pairs = [
        (['ë“±ë¡ê¸ˆ', 'í•™ë¹„', 'ë‚©ë¶€ê¸ˆ'], ['ë“±ë¡ê¸ˆ', 'ë‚©ë¶€']),
        (['ìˆ˜ê°•ì‹ ì²­', 'ìˆ˜ê°•', 'ê°•ì˜ì‹ ì²­'], ['ìˆ˜ê°•ì‹ ì²­', 'ìˆ˜ì—…ê´€ë¦¬']),
        (['ì„±ì ', 'í•™ì ', 'ì ìˆ˜'], ['ì„±ì ']),
        (['ì¡¸ì—…', 'ì¡¸ì—…ìš”ê±´', 'í•™ìœ„'], ['ì¡¸ì—…']),
        (['íœ´í•™', 'íœ´í•™ì‹ ì²­'], ['íœ´í•™']),
        (['ë³µí•™', 'ë³µí•™ì‹ ì²­'], ['ë³µí•™']),
        (['ì¥í•™ê¸ˆ', 'ì¥í•™', 'ì§€ì›ê¸ˆ'], ['ì¥í•™ê¸ˆ']),
        (['ì·¨ì—…', 'ì·¨ì—…ì§€ì›', 'ì¼ìë¦¬'], ['ì·¨ì—…', 'ì§„ë¡œ']),
        (['ì…í•™', 'ì…ì‹œ', 'ì‹ ì…ìƒ'], ['ì…í•™', 'ì…ì‹œ', 'ëª¨ì§‘']),
        (['ì‹¤ìŠµ', 'í˜„ì¥ì‹¤ìŠµ', 'ì¸í„´ì‹­'], ['í˜„ì¥ì‹¤ìŠµ', 'ì‹¤í—˜ì‹¤ìŠµ']),
        (['ìƒë‹´', 'ì‹¬ë¦¬ìƒë‹´', 'í•™ìƒìƒë‹´'], ['ì‹¬ë¦¬ìƒë‹´', 'í•™ìƒìƒë‹´']),
        (['ì‹œì„¤', 'ê±´ë¬¼', 'ê³µì‚¬'], ['ì‹œì„¤', 'ê³µì‚¬']),
        (['ì¸ì‚¬', 'ì¸ì‚¬ê´€ë¦¬', 'ì§ì›'], ['ì¸ì‚¬']),
        (['ì˜ˆì‚°', 'íšŒê³„', 'ì¬ì •'], ['ì˜ˆì‚°', 'íšŒê³„'])
    ]
    
    for word_group, keyword_group in similar_pairs:
        if word in word_group and keyword in keyword_group:
            return True
    return False

def display_search_results(search_results):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê´€ë ¨ì„± ì ìˆ˜ì™€ í•¨ê»˜ ëª…í™•í•˜ê²Œ í‘œì‹œí•©ë‹ˆë‹¤."""
    if not search_results:
        return
    
    st.write(f"ğŸ¯ **ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ê´€ë ¨ í•­ëª© ë°œê²¬**")
    
    # ê´€ë ¨ì„± ì ìˆ˜ í†µê³„ í‘œì‹œ
    scores = [doc.metadata.get('relevance_score', 0.0) for doc in search_results]
    if scores:
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í‰ê·  ê´€ë ¨ì„±", f"{avg_score:.1%}")
        with col2:
            st.metric("ìµœê³  ê´€ë ¨ì„±", f"{max_score:.1%}")
        with col3:
            st.metric("ìµœì € ê´€ë ¨ì„±", f"{min_score:.1%}")
    
    st.write("")
    
    # í•­ëª©ë³„ë¡œ ê´€ë ¨ì„± ì ìˆ˜ì™€ í•¨ê»˜ í‘œì‹œ
    for i, doc in enumerate(search_results, 1):
        title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
        date = doc.metadata.get('date', 'N/A')
        source = doc.metadata.get('filename', 'N/A')
        relevance_score = doc.metadata.get('relevance_score', 0.0)
        
        # ê´€ë ¨ì„± ì§€ì‹œì
        indicator, level, alert_type = get_relevance_indicator(relevance_score)
        
        # ê´€ë ¨ì„±ì— ë”°ë¥¸ ìŠ¤íƒ€ì¼ë§
        if alert_type == "success":
            container_type = st.success
        elif alert_type == "warning":
            container_type = st.warning
        else:
            container_type = st.error
            
        # í¼ì³ì§€ëŠ” ë°•ìŠ¤ë¡œ í‘œì‹œ (ê´€ë ¨ì„± ì ìˆ˜ í¬í•¨)
        with st.expander(f"{indicator} **í•­ëª© {i}**: {title} | ê´€ë ¨ì„±: {relevance_score:.1%} ({level})"):
            # ê´€ë ¨ì„± ì ìˆ˜ì— ë”°ë¥¸ ì¶”ê°€ ì•ˆë‚´
            if relevance_score >= 0.85:
                st.success(f"ğŸ¯ **ë†’ì€ ê´€ë ¨ì„±** ({relevance_score:.1%}) - ë§¤ìš° ì‹ ë¢°í•  ë§Œí•œ ì •ë³´ì…ë‹ˆë‹¤.")
            elif relevance_score >= 0.60:
                st.warning(f"âš ï¸ **ë³´í†µ ê´€ë ¨ì„±** ({relevance_score:.1%}) - ì°¸ê³ ìš©ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ **ë‚®ì€ ê´€ë ¨ì„±** ({relevance_score:.1%}) - ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
            
            st.write(f"**ğŸ“… ë‚ ì§œ**: {date}")
            st.write(f"**ğŸ“‚ ì¶œì²˜**: {source}")
            st.write("**ğŸ“„ ë‚´ìš©**:")
            
            # ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ë‚´ìš©ì„ ì¶•ì•½í•´ì„œ í‘œì‹œ
            if relevance_score < 0.60:
                preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                st.text(preview)
                st.caption("âš ï¸ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì¶•ì•½ëœ ë‚´ìš©ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
            else:
                preview = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                st.text(preview)

    # ì „ì²´ì ì¸ ê´€ë ¨ì„± ê²½ê³ 
    if scores and max(scores) < 0.60:
        st.error("âš ï¸ **ì£¼ì˜**: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ë¶€ì„œ ë¬¸ì˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    elif scores and avg_score < 0.50:
        st.warning("âš ï¸ **ì£¼ì˜**: í‰ê·  ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë‹¤ë¥´ê²Œ ì‹œë„í•´ë³´ì„¸ìš”.")

def generate_ai_response(query, bedrock, search_results=None):
    """AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."""
    try:
        if search_results and len(search_results) > 0:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° RAG ì‘ë‹µ
            context = "\n".join([doc.page_content for doc in search_results])
            
            # ì¶œì²˜ ì •ë³´ ì •ë¦¬
            sources = []
            for i, doc in enumerate(search_results, 1):
                title = doc.metadata.get('title', 'N/A')
                date = doc.metadata.get('date', 'N/A')
                sources.append(f"[í•­ëª© {i}] {title} ({date})")
            
            sources_text = "\n".join(sources)
            
            prompt = f"""ë‹¤ìŒì€ í•™ì‚¬ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
{context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ê³ , í™•ì‹¤í•œ ì •ë³´ë§Œ ë‹µë³€ì— í¬í•¨í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì ˆì°¨ë‚˜ ì¡°ê±´ë„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.

ì¤‘ìš”: ë‹µë³€ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì°¸ê³  ìë£Œë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”:

ğŸ“‹ **ì°¸ê³  ìë£Œ:**
{sources_text}"""
        else:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ AI ì‘ë‹µ
            prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë©´ì„œ ë„ì›€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë§Œì•½ í•™ì‚¬ ì •ë³´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë¼ë©´, í˜„ì¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ì•ˆë‚´í•˜ê³  
ì—°ì„±ëŒ€í•™êµ ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•˜ë¼ê³  ì•ˆë‚´í•´ì£¼ì„¸ìš”."""
        
        # boto3 bedrock-runtimeì—ì„œ Nova ëª¨ë¸ì˜ ì •í™•í•œ í˜•ì‹ ì‚¬ìš©
        response = bedrock.invoke_model(
            modelId="us.amazon.nova-lite-v1:0",
            contentType="application/json",
            accept="application/json",
            body=json.dumps({
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "inferenceConfig": {
                    "maxTokens": 4000,
                    "temperature": 0.7
                }
            })
        )
        
        # ì‘ë‹µ íŒŒì‹± - Nova ëª¨ë¸ì˜ ì‘ë‹µ í˜•ì‹
        response_body = json.loads(response['body'].read())
        
        # Nova ì‘ë‹µ êµ¬ì¡°: {"output": {"message": {"content": [{"text": "ë‹µë³€"}]}}}
        if 'output' in response_body and 'message' in response_body['output']:
            content = response_body['output']['message'].get('content', [])
            if content and len(content) > 0:
                return content[0].get('text', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        
        return response_body.get('outputText', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# S3 íŒŒì¼ê³¼ DB ë©”íƒ€ë°ì´í„°ë¥¼ ë™ì‹œì— ì‚­ì œí•˜ëŠ” í•¨ìˆ˜ (delete_document_from_db í•¨ìˆ˜ê°€ document_idë¡œ ì‚­ì œí•˜ë„ë¡ ìœ„ì— ìˆ˜ì •ë¨)
def delete_file_from_s3_and_db(engine, s3_client, bucket_name, document_id):
    """DBì— ìˆëŠ” ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì™€ S3ì˜ ì‹¤ì œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        # 1. DBì—ì„œ source_url (S3 key) ì¡°íšŒ
        with engine.connect() as conn:
            result = conn.execute(text("SELECT source_url FROM documents WHERE id = :document_id"),
                                  {"document_id": document_id}).fetchone()
            
            if not result:
                st.error("DBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
            source_url = result[0]
            
            # 2. S3 Key ì¶”ì¶œ
            s3_key = source_url.replace(f"s3://{bucket_name}/", "")
            
            # 3. S3 íŒŒì¼ ì‚­ì œ
            if s3_key and not source_url.startswith("rss"): # RSSëŠ” S3 íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                 s3_client.delete_object(Bucket=bucket_name, Key=s3_key)
                 st.info(f"S3 íŒŒì¼ ì‚­ì œ: {s3_key}")
            
            # 4. DBì—ì„œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë° ì²­í¬ ì‚­ì œ
            chunks_deleted, docs_deleted = delete_document_from_db(document_id, engine)

            if docs_deleted > 0:
                return True
            else:
                st.error("DB ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨")
                return False
                
    except Exception as e:
        st.error(f"S3 ë° DB ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False


# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    bedrock_client, embeddings, s3_client = init_aws_clients()
    if not bedrock_client:
        st.error("Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. EC2 IAM ì—­í• ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # PostgreSQL ì—”ì§„ ì´ˆê¸°í™”
    engine = init_postgresql_vectorstore()
    if not engine:
        return
    
    # í•™êµ ì„ íƒ UI (ì „ì²´ ì•± ìƒë‹¨)
    school_id, selected_school = render_school_selector(engine)
    
    # ì„ íƒëœ í•™êµ í†µê³„ í‘œì‹œ
    stats = get_school_stats(engine, school_id)
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ“„ ì´ ë¬¸ì„œ", stats["total_documents"])
    with col2:
        st.metric("âœ… ì²˜ë¦¬ ì™„ë£Œ", stats["processed_documents"])
    with col3:
        st.metric("ğŸ“Š ì´ ì²­í¬", stats["total_chunks"])
    
    st.divider()
    
    # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš°)
    vectorstore = init_pgvector(embeddings, engine) if embeddings else None
    
    st.success("âœ… ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    if not vectorstore:
        st.info("ğŸ’¬ ì¼ë°˜ AI ì±—ë´‡ + í…ìŠ¤íŠ¸ ê²€ìƒ‰ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        st.success("ğŸ” ë²¡í„° ê²€ìƒ‰ + AI ì±—ë´‡ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ’¬ ì±—ë´‡", "ğŸ“„ PDF ì—…ë¡œë“œ", "ğŸ“¡ S3 PDF ê´€ë¦¬", "ğŸ”— RSS í”¼ë“œ", "ğŸ“Š íŒŒì¼ ê´€ë¦¬"
    ])
    
    # íƒ­ 1: ì±—ë´‡
    with tab1:
        st.header("ğŸ’¬ í•™ì‚¬ ì •ë³´ ì±—ë´‡")
        st.info(f"ğŸ“š **{selected_school}** ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤!")
        
        search_query = st.text_input(
            "ê¶ê¸ˆí•œ ë‚´ìš©ì„ ìì—°ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ì¥í•™ê¸ˆ ì‹ ì²­ ë°©ë²• / ì…í•™ ê´€ë ¨ ì •ë³´ / ì¡¸ì—… ìš”ê±´",
            key=f"search_query_{school_id}"
        )
        
        if search_query:
            with st.spinner("ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                # ì„ íƒëœ í•™êµì˜ ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰
                search_results = search_documents(search_query, vectorstore, engine, school_id)
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
                high_quality_results = []
                if search_results:
                    for doc in search_results:
                        relevance_score = doc.metadata.get('relevance_score', 0.0)
                        if relevance_score >= 0.50:  # ì„ê³„ê°’: 50%ë¡œ í•˜í–¥ ì¡°ì •
                            high_quality_results.append(doc)
                
                if high_quality_results:
                    # RAG ê¸°ë°˜ ì‘ë‹µ (ê³ í’ˆì§ˆ ê²°ê³¼ë§Œ)
                    st.success("ğŸ“š ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€")
                    
                    # í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ í‘œì‹œ
                    avg_relevance = sum(doc.metadata.get('relevance_score', 0.0) for doc in high_quality_results) / len(high_quality_results)
                    st.info(f"ğŸ“Š í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.1%} | ì´ {len(high_quality_results)}ê°œ ë¬¸ì„œ ì°¸ì¡°")
                    
                    display_search_results(high_quality_results)
                    st.write("---")
                    
                    # AI ì‘ë‹µ ìƒì„±
                    ai_response = generate_ai_response(search_query, bedrock_client, high_quality_results)
                    st.subheader("ğŸ¤– AI ì‘ë‹µ")
                    st.markdown(ai_response)
                
                elif search_results:
                    # ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì§€ë§Œ ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš°
                    st.warning("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤")
                    
                    avg_relevance = sum(doc.metadata.get('relevance_score', 0.0) for doc in search_results) / len(search_results)
                    st.write(f"ğŸ“Š í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.1%} (ì„ê³„ê°’: 60% ë¯¸ë§Œ)")
                    
                    # ë‚®ì€ ê´€ë ¨ì„± ê²°ê³¼ë„ ì°¸ê³ ìš©ìœ¼ë¡œ í‘œì‹œ
                    with st.expander("ğŸ” ë‚®ì€ ê´€ë ¨ì„± ê²€ìƒ‰ ê²°ê³¼ (ì°¸ê³ ìš©)"):
                        display_search_results(search_results)
                    
                    # Fallback: ë¶€ì„œ ê²€ìƒ‰
                    department = find_relevant_department(search_query, school_id, engine)
                    
                    if department:
                        # ë¶€ì„œ ë§¤ì¹­ëœ ê²½ìš°
                        st.info("ğŸ“ ë‹´ë‹¹ ë¶€ì„œ ì•ˆë‚´")
                        
                        contact_info = f"ğŸ“ **{department['name']}**\n"
                        
                        if department['staff_name']:
                            contact_info += f"â€¢ ë‹´ë‹¹ì: {department['staff_name']} ({department['staff_position']})\n"
                        
                        if department['staff_phone']:
                            contact_info += f"â€¢ ì „í™”ë²ˆí˜¸: {department['staff_phone']}\n"
                        elif department['main_phone']:
                            contact_info += f"â€¢ ëŒ€í‘œë²ˆí˜¸: {department['main_phone']}\n"
                            
                        if department['staff_email']:
                            contact_info += f"â€¢ ì´ë©”ì¼: {department['staff_email']}\n"
                            
                        if department['staff_responsibilities']:
                            contact_info += f"â€¢ ë‹´ë‹¹ì—…ë¬´: {department['staff_responsibilities']}\n"
                        
                        contact_info += f"â€¢ ì—…ë¬´ì‹œê°„: í‰ì¼ 9ì‹œ~18ì‹œ"
                        
                        fallback_response = f"""ğŸ“š **'{search_query}'**ì— ëŒ€í•œ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ê´€ë ¨ ì—…ë¬´ëŠ” ë‹¤ìŒ ë¶€ì„œì—ì„œ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤:

{contact_info}

ì´ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œë©´ ë” ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
                        
                        st.markdown(fallback_response)
                        st.caption("ğŸ’¡ ê´€ë ¨ì„±ì´ ë‚®ì€ ë¬¸ì„œë³´ë‹¤ëŠ” í•´ë‹¹ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
                        
                    else:
                        # ë§¤ì¹­ë˜ëŠ” ë¶€ì„œë„ ì—†ëŠ” ê²½ìš°
                        general_response = f"""ğŸ“š **'{search_query}'**ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**{selected_school}** ì¼ë°˜ í•™ì‚¬ ë¬¸ì˜ëŠ” ë‹¤ìŒìœ¼ë¡œ ì—°ë½í•´ì£¼ì„¸ìš”:

ğŸ“ **êµë¬´ì²˜ (í•™ì‚¬ì—…ë¬´ ì´ê´„)**
â€¢ ì „í™”ë²ˆí˜¸: 441-1066
â€¢ ì´ë©”ì¼: hhlee@yeonsung.ac.kr
â€¢ ì—…ë¬´ì‹œê°„: í‰ì¼ 9ì‹œ~18ì‹œ

ë˜ëŠ” í•´ë‹¹ í•™ê³¼ ì‚¬ë¬´ì‹¤ë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
                        
                        st.markdown(general_response)
                        st.caption("ğŸ’¡ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì€ ê´€ë ¨ í•™ê³¼ë‚˜ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ë°”ë¡œ Fallback
                    department = find_relevant_department(search_query, school_id, engine)
                    
                    if department:
                        # ë¶€ì„œ ë§¤ì¹­ëœ ê²½ìš°
                        st.info("ğŸ“ ë‹´ë‹¹ ë¶€ì„œ ì•ˆë‚´")
                        
                        contact_info = f"ğŸ“ **{department['name']}**\n"
                        
                        if department['staff_name']:
                            contact_info += f"â€¢ ë‹´ë‹¹ì: {department['staff_name']} ({department['staff_position']})\n"
                        
                        if department['staff_phone']:
                            contact_info += f"â€¢ ì „í™”ë²ˆí˜¸: {department['staff_phone']}\n"
                        elif department['main_phone']:
                            contact_info += f"â€¢ ëŒ€í‘œë²ˆí˜¸: {department['main_phone']}\n"
                            
                        if department['staff_email']:
                            contact_info += f"â€¢ ì´ë©”ì¼: {department['staff_email']}\n"
                            
                        if department['staff_responsibilities']:
                            contact_info += f"â€¢ ë‹´ë‹¹ì—…ë¬´: {department['staff_responsibilities']}\n"
                        
                        contact_info += f"â€¢ ì—…ë¬´ì‹œê°„: í‰ì¼ 9ì‹œ~18ì‹œ"
                        
                        fallback_response = f"""ğŸ“š **'{search_query}'**ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ê´€ë ¨ ì—…ë¬´ëŠ” ë‹¤ìŒ ë¶€ì„œì—ì„œ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤:

{contact_info}

ì´ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œë©´ ë” ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
                        
                        st.markdown(fallback_response)
                        st.caption("ğŸ’¡ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì€ í•´ë‹¹ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
                        
                    else:
                        # ë§¤ì¹­ë˜ëŠ” ë¶€ì„œê°€ ì—†ëŠ” ê²½ìš°
                        st.warning("â“ ì¼ë°˜ ë¬¸ì˜ ì•ˆë‚´")
                        
                        general_response = f"""ğŸ“š **'{search_query}'**ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**{selected_school}** ì¼ë°˜ í•™ì‚¬ ë¬¸ì˜ëŠ” ë‹¤ìŒìœ¼ë¡œ ì—°ë½í•´ì£¼ì„¸ìš”:

ğŸ“ **êµë¬´ì²˜ (í•™ì‚¬ì—…ë¬´ ì´ê´„)**
â€¢ ì „í™”ë²ˆí˜¸: 441-1066
â€¢ ì´ë©”ì¼: hhlee@yeonsung.ac.kr
â€¢ ì—…ë¬´ì‹œê°„: í‰ì¼ 9ì‹œ~18ì‹œ

ë˜ëŠ” í•´ë‹¹ í•™ê³¼ ì‚¬ë¬´ì‹¤ë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
                        
                        st.markdown(general_response)
                        st.caption("ğŸ’¡ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì€ ê´€ë ¨ í•™ê³¼ë‚˜ ë¶€ì„œë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    
    # íƒ­ 2: PDF ì—…ë¡œë“œ
    with tab2:
        st.header("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ")
        st.info(f"ğŸ¤– **{selected_school}**ì— ìë™ìœ¼ë¡œ ì—…ë¡œë“œë©ë‹ˆë‹¤. PDFê°€ Lambda í•¨ìˆ˜ì— ì˜í•´ ìë™ ë²¡í„°í™” ì²˜ë¦¬ë©ë‹ˆë‹¤!")
        
        uploaded_files = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ ì‹œì‘)",
            type=['pdf'],
            accept_multiple_files=True,
            key=f"pdf_uploader_{school_id}_{st.session_state.get(f'uploader_reset_{school_id}', 0)}"  # ë™ì  í‚¤
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.write(f"íŒŒì¼ëª…: {uploaded_file.name}")
                    st.write(f"í¬ê¸°: {uploaded_file.size:,} bytes")
                
                with col2:
                    if st.button(f"ì—…ë¡œë“œ", key=f"upload_{uploaded_file.name}_{school_id}"):
                        with st.spinner("ì—…ë¡œë“œ ì¤‘..."):
                            # í•™êµë³„ S3 í‚¤ ìƒì„±
                            school_code = get_school_code_by_id(engine, school_id)
                            s3_key = f"documents/{school_code}/{datetime.now().strftime('%Y/%m/%d')}/{uploaded_file.name}"
                            
                            # S3 ì—…ë¡œë“œ
                            if upload_to_s3(uploaded_file, s3_client, S3_BUCKET_NAME, s3_key):
                                # ë©”íƒ€ë°ì´í„° ì €ì¥ (school_id í¬í•¨)
                                if save_file_metadata(engine, uploaded_file.name, s3_key, "pdf", school_id):
                                    st.success(f"âœ… {uploaded_file.name} ì—…ë¡œë“œ ì™„ë£Œ!")
                                    st.info("ğŸ¤– **ìë™ ì²˜ë¦¬**: PDFê°€ Lambda í•¨ìˆ˜ì— ì˜í•´ ìë™ìœ¼ë¡œ ë²¡í„°í™” ì²˜ë¦¬ë©ë‹ˆë‹¤.")
                                    st.caption("ğŸ’¡ S3 PDF ì²˜ë¦¬ íƒ­ì—ì„œ ì²˜ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                    
                                    # 2ì´ˆ í›„ ìë™ ìƒˆë¡œê³ ì¹¨
                                    st.info("ğŸ”„ 2ì´ˆ í›„ í˜ì´ì§€ê°€ ìë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ë©ë‹ˆë‹¤...")
                                    import time
                                    time.sleep(2)
                                    
                                    # file_uploader ì™„ì „ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ í‚¤ ë¦¬ì…‹
                                    current_reset = st.session_state.get(f'uploader_reset_{school_id}', 0)
                                    st.session_state[f'uploader_reset_{school_id}'] = current_reset + 1
                                    
                                    # ê¸°ì¡´ ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì •ë¦¬
                                    keys_to_delete = []
                                    for key in st.session_state.keys():
                                        if f"pdf_uploader_{school_id}_" in str(key) and str(current_reset) in str(key):
                                            keys_to_delete.append(key)
                                    
                                    for key in keys_to_delete:
                                        del st.session_state[key]
                                    
                                    st.rerun()
                                else:
                                    st.error("ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                            else:
                                st.error("S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # íƒ­ 3: S3 PDF ê´€ë¦¬
    with tab3:
        st.header(f"ğŸ“¡ S3 PDF ê´€ë¦¬ - {selected_school}")
        st.info("ì—…ë¡œë“œëœ PDF íŒŒì¼ë“¤ì„ ë²¡í„°í™”í•˜ê±°ë‚˜ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ëª©ë¡ ì¡°íšŒ (school_id í•„í„°ë§ ì ìš©)
        file_metadata = get_file_metadata(engine, school_id)
        
        if not file_metadata.empty:
            st.subheader("ğŸ“Š ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
            page_size = 5  # í•œ í˜ì´ì§€ë‹¹ íŒŒì¼ ìˆ˜
            total_files = len(file_metadata)
            total_pages = (total_files - 1) // page_size + 1
            
            # í˜ì´ì§€ ì„ íƒ UI
            if total_pages > 1:
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    current_page = st.selectbox(
                        "í˜ì´ì§€ ì„ íƒ",
                        range(1, total_pages + 1),
                        format_func=lambda x: f"í˜ì´ì§€ {x} / {total_pages}",
                        key=f"pdf_page_{school_id}"
                    )
                    st.caption(f"ì´ {total_files}ê°œ íŒŒì¼ ì¤‘ {(current_page-1)*page_size + 1}~{min(current_page*page_size, total_files)}ê°œ í‘œì‹œ")
            else:
                current_page = 1
            
            # í˜„ì¬ í˜ì´ì§€ íŒŒì¼ë“¤ ì¶”ì¶œ
            start_idx = (current_page - 1) * page_size
            end_idx = min(start_idx + page_size, total_files)
            current_page_files = file_metadata.iloc[start_idx:end_idx]
            
            # í‘œì‹œìš© DataFrame ìƒì„± (í˜„ì¬ í˜ì´ì§€ë§Œ)
            display_df = current_page_files.copy()
            display_df['ìƒíƒœ'] = display_df['processed'].apply(lambda x: 'âœ… ì²˜ë¦¬ì™„ë£Œ' if x else 'â³ ë¯¸ì²˜ë¦¬')
            display_df['ì²­í¬ìˆ˜'] = display_df['chunks_count'].astype(int)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì»¬ëŸ¼ëª… í•œêµ­ì–´ë¡œ ë³€ê²½
            display_columns = {
                'filename': 'íŒŒì¼ëª…',
                's3_key': 'S3 í‚¤',
                'upload_date': 'ì—…ë¡œë“œì¼',
                'ìƒíƒœ': 'ìƒíƒœ',
                'ì²­í¬ìˆ˜': 'ì²­í¬ìˆ˜'
            }
            
            # ê¹”ë”í•œ í‘œë¡œ í‘œì‹œ (í˜„ì¬ í˜ì´ì§€ë§Œ)
            display_table = display_df[list(display_columns.keys())].rename(columns=display_columns)
            st.dataframe(display_table, use_container_width=True, hide_index=True)
            
            # íŒŒì¼ë³„ ê´€ë¦¬ ë²„íŠ¼ë“¤ (í˜„ì¬ í˜ì´ì§€ë§Œ)
            st.subheader("ğŸ—‘ï¸ íŒŒì¼ ê´€ë¦¬")
            
            for page_idx, (idx, row) in enumerate(current_page_files.iterrows()):
                # ê³ ìœ í•œ í‚¤ ìƒì„± (í˜ì´ì§€ + ì¸ë±ìŠ¤)
                unique_key = f"page_{current_page}_item_{page_idx}"
                
                col1, col2, col3 = st.columns([4, 2, 1])
                
                with col1:
                    status_icon = "âœ…" if row['processed'] else "â³"
                    st.write(f"{status_icon} **{row['filename']}**")
                
                with col2:
                    if row['processed']:
                        st.success(f"{row['chunks_count']}ê°œ ì²­í¬")
                    else:
                        st.warning("ë¯¸ì²˜ë¦¬")
                
                with col3:
                    # ì‚­ì œ í™•ì¸ ìƒíƒœ
                    delete_key = f"delete_confirm_{unique_key}"
                    if delete_key not in st.session_state:
                        st.session_state[delete_key] = False
                    
                    if not st.session_state[delete_key]:
                        if st.button("ğŸ—‘ï¸", key=f"delete_btn_{unique_key}", help="íŒŒì¼ ì‚­ì œ"):
                            st.session_state[delete_key] = True
                            st.rerun()
                    else:
                        # ì‚­ì œ í™•ì¸ ëª¨ë“œ
                        subcol1, subcol2 = st.columns(2)
                        with subcol1:
                            if st.button("âœ…", key=f"confirm_{unique_key}", use_container_width=True, type="primary", help="ì‚­ì œ í™•ì¸"):
                                with st.spinner("ì‚­ì œ ì¤‘..."):
                                    if delete_file_from_s3_and_db(engine, s3_client, S3_BUCKET_NAME, row['id']):
                                        st.success("âœ… íŒŒì¼ ì‚­ì œ ì™„ë£Œ!")
                                        if delete_key in st.session_state:
                                            del st.session_state[delete_key]
                                        st.rerun()
                                    else:
                                        st.error("ì‚­ì œ ì‹¤íŒ¨")
                        with subcol2:
                            if st.button("âŒ", key=f"cancel_{unique_key}", use_container_width=True, help="ì‚­ì œ ì·¨ì†Œ"):
                                st.session_state[delete_key] = False
                                st.rerun()
                
                st.divider()
                # --- IndentationError ë°œìƒ ì§€ì  ìˆ˜ì •:
                # ì´ ë¡œì§ì€ col3ê°€ ëë‚œ í›„, í•˜ì§€ë§Œ for ë£¨í”„ê°€ ëë‚˜ê¸° ì „ì— ì™€ì•¼ í•©ë‹ˆë‹¤.
                # ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ì½”ë“œë¥¼ ë³´ë©´ ì´ ë¡œì§ì´ ì¤‘ë³µëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
                # ì›ë³¸ ì½”ë“œì˜ ì˜ë„ë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ 1349 ë¼ì¸ ì£¼ë³€ì˜ ì¤‘ë³µë˜ëŠ” ì‚­ì œ í™•ì¸ ë¡œì§ì„ ì‚­ì œí•©ë‹ˆë‹¤.
                # ì›ë³¸ ì½”ë“œì˜ 1300 ë¼ì¸ë¶€í„° 1345 ë¼ì¸ê¹Œì§€ ì´ë¯¸ ì‚­ì œ ë¡œì§ì´ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
                # ì´ ë¶€ë¶„(1349 ë¼ì¸~)ì€ ì¤‘ë³µ/ì˜ëª»ëœ ë“¤ì—¬ì“°ê¸°ë¡œ íŒë‹¨ë˜ë¯€ë¡œ,
                # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì›ë³¸ ì½”ë“œì˜ 1349 ë¼ì¸ ì´í›„ì˜ ì‚­ì œ í™•ì¸ ë¡œì§ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°í•©ë‹ˆë‹¤.
                # ë‹¤ë§Œ, ìš”ì²­ì— ë”°ë¼ IndentationErrorë§Œ ìˆ˜ì •í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë“¤ì—¬ì“°ê¸°ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
                
                # ì›ë³¸ ì½”ë“œì˜ IndentationError ë°œìƒ ë¶€ë¶„ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•©ë‹ˆë‹¤:
                # (1349 ë¼ì¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ë¶€ë¶„ì˜ ë“¤ì—¬ì“°ê¸°ë¥¼ 3ë‹¨ê³„ë¡œ ìˆ˜ì •í•˜ì—¬ for ë£¨í”„ ì•ˆì— ìœ„ì¹˜)
                if st.session_state.get(delete_key, False):
                    st.warning("ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì¤‘ë³µ ë¡œì§)")
                    
                    col_yes, col_no = st.columns(2)
                    with col_yes:
                        if st.button("âœ…", key=f"delete_yes_{idx}", help="ì‚­ì œ í™•ì¸"):
                            with st.spinner("ì‚­ì œ ì¤‘..."):
                                # ì´ ë¡œì§ì€ 1300-1345 ë¼ì¸ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ, ì‹¤ì œë¡œëŠ” ë„ë‹¬í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
                                st.error("ì´ë¯¸ ìœ„ì— ìˆëŠ” ë²„íŠ¼ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆì–´ì•¼ í•©ë‹ˆë‹¤. ì½”ë“œë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.")
                                if delete_key in st.session_state:
                                     del st.session_state[delete_key]
                                st.rerun()
                    
                    with col_no:
                        if st.button("âŒ", key=f"delete_no_{idx}", help="ì‚­ì œ ì·¨ì†Œ"):
                            # ì‚­ì œ í™•ì¸ ìƒíƒœ í•´ì œ
                            if delete_key in st.session_state:
                                del st.session_state[delete_key]
                            st.rerun()
                
                # --- ìˆ˜ì • ë
                
                st.divider()
        else:
            st.info(f"**{selected_school}**ì— ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìˆ˜ë™ìœ¼ë¡œ S3 í‚¤ ì…ë ¥í•˜ì—¬ ì²˜ë¦¬
        st.subheader("ğŸ”§ ìˆ˜ë™ S3 íŒŒì¼ ì²˜ë¦¬")
        st.info("S3ì— ì§ì ‘ ì—…ë¡œë“œëœ íŒŒì¼ì´ë‚˜ íŠ¹ì • ê²½ë¡œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.caption("ğŸ’¡ íŒ: ìœ„ í‘œì—ì„œ S3 í‚¤ë¥¼ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤. `s3://` í˜•ì‹ë„ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            manual_s3_key = st.text_input(
                "S3 í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", 
                placeholder="ì˜ˆ: documents/2024/01/15/file.pdf ë˜ëŠ” s3://bucket/path/file.pdf",
                key="manual_s3_key"
            )
        
        with col2:
            st.write("")  # ê³µê°„ ì¡°ì •
            st.write("")  # ê³µê°„ ì¡°ì •
            if st.button("ğŸ“ ì²˜ë¦¬", disabled=not manual_s3_key):
                with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                    # S3 URIì—ì„œ ìˆœìˆ˜í•œ í‚¤ë§Œ ì¶”ì¶œ
                    clean_s3_key = manual_s3_key
                    if clean_s3_key.startswith(f"s3://{S3_BUCKET_NAME}/"):
                        clean_s3_key = clean_s3_key.replace(f"s3://{S3_BUCKET_NAME}/", "")
                    elif clean_s3_key.startswith("s3://"):
                        # ë‹¤ë¥¸ ë²„í‚· URIì¸ ê²½ìš° ë²„í‚·ëª…ë„ ì œê±°
                        clean_s3_key = "/".join(clean_s3_key.split("/")[3:])
                    
                    chunks = process_pdf_from_s3(
                        s3_client, S3_BUCKET_NAME, clean_s3_key, 
                        vectorstore, embeddings, engine
                    )
                    if chunks > 0:
                        st.success(f"âœ… {chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
                        st.rerun()
                    else:
                        st.error("ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. S3 í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # íƒ­ 4: RSS í”¼ë“œ
    with tab4:
        st.header("ğŸ”— RSS í”¼ë“œ ê´€ë¦¬")
        st.info(f"ğŸ“š **{selected_school}**ì˜ RSS í”¼ë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.")
        
        # RSS ì¶”ê°€ ì„¹ì…˜
        st.subheader("ğŸ†• ìƒˆ RSS í”¼ë“œ ì¶”ê°€")
        col1, col2 = st.columns([4, 1])
        
        with col1:
            # URL ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
            if 'rss_url_input' not in st.session_state:
                st.session_state.rss_url_input = ""
            
            rss_url = st.text_input(
                "RSS í”¼ë“œ URLì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="https://example.com/rss",
                key="new_rss_url",
                value=st.session_state.rss_url_input
            )
        
        with col2:
            st.write("")  # ê³µê°„ ì¡°ì •
            st.write("")  # ê³µê°„ ì¡°ì •
            if st.button("â• ì¶”ê°€", disabled=not rss_url):
                with st.spinner("RSS í”¼ë“œë¥¼ ì¶”ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    rss_feed_id = add_rss_feed(engine, school_id, rss_url)
                    if rss_feed_id:
                        st.success("âœ… RSS í”¼ë“œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        # URL ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
                        st.session_state.rss_url_input = ""
                        st.rerun()
                    else:
                        st.warning("ì´ë¯¸ ë“±ë¡ëœ RSS í”¼ë“œì´ê±°ë‚˜ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        st.divider()
        
        # ë“±ë¡ëœ RSS í”¼ë“œ ëª©ë¡
        st.subheader("ğŸ“¡ ë“±ë¡ëœ RSS í”¼ë“œ ëª©ë¡")
        rss_feeds = get_rss_feeds(engine, school_id)
        
        if not rss_feeds.empty:
            # í†µê³„ í‘œì‹œ
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ë“±ë¡ëœ í”¼ë“œ", len(rss_feeds))
            with col2:
                active_feeds = len(rss_feeds[rss_feeds['status'] == 'active'])
                st.metric("í™œì„± í”¼ë“œ", active_feeds)
            with col3:
                total_items = rss_feeds['processed_count'].sum()
                st.metric("ì²˜ë¦¬ëœ í•­ëª©", int(total_items))
            
            st.write("")
            
            # RSS í”¼ë“œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œìš© ë°ì´í„° ì¤€ë¹„
            display_df = rss_feeds.copy()
            
            # í‘œì‹œìš© ì»¬ëŸ¼ ì •ë¦¬
            display_df['í”¼ë“œëª…'] = display_df['title'].fillna('ì œëª© ì—†ìŒ')
            display_df['RSS URL'] = display_df['rss_url'].apply(
                lambda x: f"{x[:60]}..." if len(x) > 60 else x
            )
            display_df['ìƒíƒœ'] = display_df['status'].apply(
                lambda x: "âœ… í™œì„±" if x == 'active' else "â¸ï¸ ë¹„í™œì„±"
            )
            display_df['ì²˜ë¦¬ëœ í•­ëª©'] = display_df['processed_count'].astype(int)
            display_df['ë§ˆì§€ë§‰ ì²˜ë¦¬'] = display_df['last_processed'].apply(
                lambda x: x.strftime('%m-%d %H:%M') if pd.notnull(x) else 'ë¯¸ì²˜ë¦¬'
            )
            display_df['ë“±ë¡ì¼'] = display_df['created_at'].apply(
                lambda x: x.strftime('%m-%d') if pd.notnull(x) else 'N/A'
            )
            
            # í‘œì‹œí•  ì»¬ëŸ¼ë§Œ ì„ íƒ
            show_columns = ['í”¼ë“œëª…', 'RSS URL', 'ìƒíƒœ', 'ì²˜ë¦¬ëœ í•­ëª©', 'ë§ˆì§€ë§‰ ì²˜ë¦¬', 'ë“±ë¡ì¼']
            
            # ê¹”ë”í•œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
            st.dataframe(
                display_df[show_columns], 
                use_container_width=True,
                hide_index=True
            )
            
            st.write("---")
            
            # ê° RSS í”¼ë“œë³„ ì•¡ì…˜ ë° ë¯¸ë¦¬ë³´ê¸°
            st.subheader("ğŸ”§ RSS í”¼ë“œ ê´€ë¦¬")
            
            for idx, row in rss_feeds.iterrows():
                with st.container():
                    # RSS ê¸°ë³¸ ì •ë³´ì™€ ë²„íŠ¼ë“¤
                    col1, col2, col3, col4 = st.columns([4, 1.5, 1.5, 1])
                    
                    with col1:
                        title = row['title'] if row['title'] else "ì œëª© ì—†ìŒ"
                        status_icon = "âœ…" if row['status'] == 'active' else "â¸ï¸"
                        st.write(f"{status_icon} **{title}**")
                        st.caption(f"ğŸ”— {row['rss_url'][:70]}...")
                        st.caption(f"ğŸ“Š {int(row['processed_count'])}ê°œ í•­ëª©")
                    
                    with col2:
                        # ë¯¸ë¦¬ë³´ê¸° ë²„íŠ¼
                        preview_key = f"preview_{row['id']}"
                        preview_text = "ğŸ“‹ ë¯¸ë¦¬ë³´ê¸° ë‹«ê¸°" if st.session_state.get(preview_key, False) else "ğŸ“‹ ë¯¸ë¦¬ë³´ê¸°"
                        
                        if st.button(preview_text, key=f"preview_btn_{row['id']}", use_container_width=True):
                            if preview_key not in st.session_state:
                                st.session_state[preview_key] = False
                            st.session_state[preview_key] = not st.session_state[preview_key]
                            st.rerun()
                    
                    with col3:
                        # ì²˜ë¦¬ ë²„íŠ¼
                        if st.button(f"ğŸ”„ ì²˜ë¦¬", key=f"process_{row['id']}", use_container_width=True):
                            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                                chunks = process_rss_feed(row['rss_url'], vectorstore, engine, embeddings, school_id)
                                if chunks > 0:
                                    st.success(f"âœ… {chunks}ê°œ í•­ëª© ì²˜ë¦¬ ì™„ë£Œ!")
                                    st.rerun()
                                else:
                                    st.info("ìƒˆë¡œìš´ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
                    with col4:
                        # ì‚­ì œ ë²„íŠ¼
                        delete_key = f"delete_rss_{row['id']}"
                        if delete_key not in st.session_state:
                            st.session_state[delete_key] = False
                        
                        if not st.session_state[delete_key]:
                            if st.button("ğŸ—‘ï¸", key=f"rss_delete_btn_{row['id']}", use_container_width=True, type="secondary", help="RSS í”¼ë“œ ì‚­ì œ"):
                                st.session_state[delete_key] = True
                                st.rerun()
                        else:
                            # ì‚­ì œ í™•ì¸ ëª¨ë“œ - ê°™ì€ ì»¬ëŸ¼ì—ì„œ í™•ì¸/ì·¨ì†Œ ë²„íŠ¼
                            subcol1, subcol2 = st.columns(2)
                            with subcol1:
                                if st.button("âœ…", key=f"rss_confirm_{row['id']}", use_container_width=True, type="primary", help="ì‚­ì œ í™•ì¸"):
                                    with st.spinner("ì‚­ì œ ì¤‘..."):
                                        if delete_rss_feed(engine, row['id']):
                                            st.success("âœ… ì‚­ì œ ì™„ë£Œ!")
                                            # URL ì…ë ¥ í•„ë“œë„ ì´ˆê¸°í™”
                                            st.session_state.rss_url_input = ""
                                            # ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì •ë¦¬
                                            if delete_key in st.session_state:
                                                del st.session_state[delete_key]
                                            preview_key = f"preview_{row['id']}"
                                            if preview_key in st.session_state:
                                                del st.session_state[preview_key]
                                            st.rerun()
                                        else:
                                            st.error("ì‚­ì œ ì‹¤íŒ¨")
                            with subcol2:
                                if st.button("âŒ", key=f"rss_cancel_{row['id']}", use_container_width=True, help="ì‚­ì œ ì·¨ì†Œ"):
                                    st.session_state[delete_key] = False
                                    st.rerun()
                    
                    # RSS ë¯¸ë¦¬ë³´ê¸° (í•´ë‹¹ í”¼ë“œ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œë§Œ í‘œì‹œ)
                    preview_key = f"preview_{row['id']}"
                    if st.session_state.get(preview_key, False):
                        with st.expander("ğŸ“‹ RSS í”¼ë“œ ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                            try:
                                feed = feedparser.parse(row['rss_url'])
                                if feed.entries:
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.metric("í”¼ë“œ ì œëª©", feed.feed.get('title', 'N/A'))
                                    with col2:
                                        st.metric("ì´ í•­ëª© ìˆ˜", len(feed.entries))
                                    
                                    # ìµœì‹  5ê°œ í•­ëª© ë¯¸ë¦¬ë³´ê¸°
                                    st.write("**ğŸ“° ìµœì‹  í•­ëª©ë“¤:**")
                                    for i, entry in enumerate(feed.entries[:5]):
                                        with st.container():
                                            st.write(f"**{i+1}. {entry.get('title', 'N/A')}**")
                                            st.write(f"ğŸ”— {entry.get('link', 'N/A')}")
                                            st.write(f"ğŸ“… {entry.get('published', 'N/A')}")
                                            summary = entry.get('summary', entry.get('description', ''))
                                            if summary:
                                                st.write(f"ğŸ“„ {summary[:200]}...")
                                            if i < 4:  # ë§ˆì§€ë§‰ í•­ëª©ì´ ì•„ë‹ˆë©´ êµ¬ë¶„ì„ 
                                                st.write("---")
                                else:
                                    st.warning("RSS í”¼ë“œì—ì„œ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"RSS í”¼ë“œ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {str(e)}")
                    
                    st.divider()
            
        else:
            st.info("ë“±ë¡ëœ RSS í”¼ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ìƒˆ RSS í”¼ë“œë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”!")
    
    # íƒ­ 5: íŒŒì¼ ê´€ë¦¬
    with tab5:
        st.header(f"ğŸ“Š íŒŒì¼ ê´€ë¦¬ - {selected_school}")
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (school_id í•„í„°ë§ ì ìš©)
        file_metadata = get_file_metadata(engine, school_id)
        
        if not file_metadata.empty:
            st.subheader("ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡")
            st.dataframe(file_metadata)
            
            # í†µê³„
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ íŒŒì¼ ìˆ˜", len(file_metadata))
            with col2:
                processed_count = len(file_metadata[file_metadata['processed'] == True])
                st.metric("ì²˜ë¦¬ëœ íŒŒì¼", processed_count)
            with col3:
                unprocessed_count = len(file_metadata[file_metadata['processed'] == False])
                st.metric("ë¯¸ì²˜ë¦¬ íŒŒì¼", unprocessed_count)
        else:
            st.info(f"**{selected_school}**ì— ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¬¸ì„œ ì²­í¬ í†µê³„ (school_id í•„í„°ë§ ì ìš©)
        st.subheader("ë¬¸ì„œ ì½˜í…ì¸  í†µê³„")
        try:
            with engine.connect() as conn:
                # school_idë¡œ í•„í„°ë§ëœ ì²­í¬ ìˆ˜
                result = conn.execute(text("""
                    SELECT COUNT(dc.*) 
                    FROM document_chunks dc 
                    JOIN documents d ON dc.document_id = d.id 
                    WHERE d.school_id = :school_id
                """), {"school_id": school_id}).fetchone()
                chunks_count = result[0] if result else 0
                st.metric("ë¬¸ì„œ ì²­í¬ ìˆ˜", chunks_count)
                
                # school_idë¡œ í•„í„°ë§ëœ ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜
                result = conn.execute(text("""
                    SELECT COUNT(DISTINCT dc.document_id) 
                    FROM document_chunks dc 
                    JOIN documents d ON dc.document_id = d.id 
                    WHERE d.school_id = :school_id
                """), {"school_id": school_id}).fetchone()
                docs_count = result[0] if result else 0
                st.metric("ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜", docs_count)
                
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ (school_id í•„í„°ë§ ì ìš©)
                result = conn.execute(text("""
                    SELECT d.category, 
                            COUNT(d.id) as total_docs,
                            SUM(CASE WHEN d.processed THEN 1 ELSE 0 END) as processed_docs,
                            SUM(COALESCE(d.chunks_count, 0)) as total_chunks
                    FROM documents d
                    WHERE d.school_id = :school_id
                    GROUP BY d.category
                    ORDER BY d.category
                """), {"school_id": school_id}).fetchall()
                
                if result:
                    st.subheader("ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í†µê³„")
                    for row in result:
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.write(f"**{row[0]}**")
                        with col2:
                            st.metric("ì´ ë¬¸ì„œ", row[1])
                        with col3:
                            st.metric("ì²˜ë¦¬ ì™„ë£Œ", row[2])
                        with col4:
                            st.metric("ì²­í¬ ìˆ˜", row[3])
                        
        except Exception as e:
            st.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    main()
