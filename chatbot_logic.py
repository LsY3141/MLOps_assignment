import streamlit as st
import json
import re
from langchain.schema import Document
from langchain_aws import ChatBedrock
from sqlalchemy import text

from database import is_similar_keyword

# --- ì±—ë´‡ í•µì‹¬ ë¡œì§ ---

def search_documents(engine, vectorstore, query, school_id, embeddings):
    """ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (by LangChain PGVector)
        if vectorstore:
            # school_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
            filter_criteria = {"school_id": school_id}
            vector_results = vectorstore.similarity_search_with_relevance_scores(
                query=query,
                k=5,
                filter=filter_criteria
            )
            # vector_resultsëŠ” (Document, score) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        else:
            vector_results = []

        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (Fallback)
        with engine.connect() as conn:
            keyword_results_raw = conn.execute(text(f"""
                SELECT dc.chunk_text, d.source_url, d.file_name, d.category, d.created_at
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.school_id = :school_id AND dc.chunk_text ILIKE :query
                ORDER BY d.created_at DESC
                LIMIT 5
            """), {"school_id": school_id, "query": f"%{query}%"}).fetchall()

        # 3. ê²°ê³¼ í†µí•© ë° Document ê°ì²´ ë³€í™˜
        processed_sources = set()
        combined_results = []

        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for doc, score in vector_results:
            if doc.metadata['source'] not in processed_sources:
                doc.metadata['relevance_score'] = score
                combined_results.append(doc)
                processed_sources.add(doc.metadata['source'])

        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ (ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ ì¤‘ë³µë˜ì§€ ì•Šê²Œ)
        for row in keyword_results_raw:
            source_url = row.source_url
            if source_url not in processed_sources:
                metadata = {
                    "source": source_url,
                    "filename": row.file_name or "RSS ê³µì§€ì‚¬í•­",
                    "category": row.category,
                    "date": row.created_at.strftime("%Y-%m-%d") if row.created_at else "N/A",
                    "title": extract_title_from_text(row.chunk_text),
                    "relevance_score": calculate_relevance_score(query, row.chunk_text, {}) # ì ìˆ˜ ë³„ë„ ê³„ì‚°
                }
                combined_results.append(Document(page_content=row.chunk_text, metadata=metadata))
                processed_sources.add(source_url)
        
        # ìµœì¢…ì ìœ¼ë¡œ ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        combined_results.sort(key=lambda x: x.metadata.get('relevance_score', 0.0), reverse=True)
        
        return combined_results[:5] # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ë°˜í™˜

    except Exception as e:
        st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return []

def generate_ai_response(bedrock_client, query, search_results):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ChatBedrock ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm = ChatBedrock(
            client=bedrock_client,
            model_id="anthropic.claude-3-sonnet-20240229-v1:0", # Sonnet ëª¨ë¸ ì‚¬ìš©
            model_kwargs={"temperature": 0.7, "max_tokens": 4000}
        )

        if search_results:
            context = "\n".join([f"<doc>{doc.page_content}</doc>" for doc in search_results])
            sources = "\n".join([f"- {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')} ({doc.metadata.get('date', 'ë‚ ì§œ ì •ë³´ ì—†ìŒ')})" for doc in search_results])
            
            prompt = f"""ë‹¹ì‹ ì€ í•™ì‚¬ ì •ë³´ ì „ë¬¸ AI ì±—ë´‡ 'ClassMATE'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ <docs> ì•ˆì˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ê³ , í™•ì‹¤í•œ ì •ë³´ë§Œ ë‹µë³€ì— í¬í•¨í•´ì£¼ì„¸ìš”.

<docs>
{context}
</docs>

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì°¸ê³  ìë£Œë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.
---
ğŸ“‹ **ì°¸ê³  ìë£Œ:**
{sources}
"""
        else:
            prompt = f"""ë‹¹ì‹ ì€ í•™ì‚¬ ì •ë³´ ì „ë¬¸ AI ì±—ë´‡ 'ClassMATE'ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì£¼ì–´ì§„ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ, ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹ , ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ì•ˆë‚´í•˜ê³  í•™êµ ê³µì‹ í™ˆí˜ì´ì§€ë‚˜ ë‹´ë‹¹ ë¶€ì„œì— ë¬¸ì˜í•˜ë¼ê³  ì¹œì ˆí•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”."""

        # LangChainì„ í†µí•´ AI ëª¨ë¸ í˜¸ì¶œ
        response = llm.invoke(prompt)
        return response.content

    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# --- í—¬í¼ í•¨ìˆ˜ (ê´€ë ¨ì„± ì ìˆ˜, í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë“±) ---

def extract_title_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('ì œëª©:'):
            return line.replace('ì œëª©:', '').strip()
        if line and 10 < len(line) < 100:
            return line
    return text[:50] + "..." if len(text) > 50 else text

def preprocess_query(query):
    """ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    stopwords = ['ì—', 'ëŒ€í•´', 'ëŒ€í•œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ê¶ê¸ˆí•©ë‹ˆë‹¤', 'ê¶ê¸ˆí•´ìš”', 'ì•Œê³ ì‹¶ì–´ìš”', 'ì•Œë ¤ì£¼ì„¸ìš”', 'ë¬¸ì˜', 'ì§ˆë¬¸', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ë¬´ì—‡', 'ì™œ', 'ì–´ë–¤', 'ì…ë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”']
    words = re.sub(r'[^\wê°€-í£\s]', ' ', query).split()
    core_keywords = [word.strip() for word in words if len(word.strip()) > 1 and word not in stopwords]
    return ' '.join(core_keywords)

def calculate_relevance_score(query, document_content, metadata):
    """ê°„ì†Œí™”ëœ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°."""
    # ì´ í•¨ìˆ˜ëŠ” ë²¡í„° ê²€ìƒ‰ì˜ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, í‚¤ì›Œë“œ ê²€ìƒ‰ fallbackì„ ìœ„í•œ ê°„ì´ ê³„ì‚°ë§Œ ìˆ˜í–‰
    processed_query = preprocess_query(query).lower()
    doc_text = document_content.lower()
    score = 0
    if processed_query in doc_text:
        score += 0.5
    
    query_words = set(processed_query.split())
    doc_words = set(doc_text.split())
    common_words = query_words.intersection(doc_words)
    score += 0.1 * len(common_words)
    
    return min(score, 1.0)

def get_relevance_indicator(score):
    """ì ìˆ˜ì— ë”°ë¥¸ ê´€ë ¨ì„± ì§€ì‹œì ì•„ì´ì½˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if score >= 0.7:
        return "âœ…", "ë†’ìŒ", "success"
    elif score >= 0.4:
        return "âš ï¸", "ë³´í†µ", "warning"
    else:
        return "âŒ", "ë‚®ìŒ", "error"
