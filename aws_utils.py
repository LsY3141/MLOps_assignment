import boto3
import streamlit as st
import tempfile
import os
import feedparser
from langchain_aws import BedrockEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from sqlalchemy import text

from config import settings

# BedrockEmbeddings í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ import
try:
    from langchain_aws import BedrockEmbeddings
except ImportError:
    try:
        from langchain_community.embeddings import BedrockEmbeddings
    except ImportError:
        BedrockEmbeddings = None

# --- AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---

@st.cache_resource
def init_aws_clients():
    """EC2 IAM ì—­í• ì„ ì‚¬ìš©í•˜ì—¬ AWS í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        bedrock_runtime_client = boto3.client("bedrock-runtime", region_name=settings.AWS_REGION)
        s3_client = boto3.client("s3", region_name=settings.AWS_REGION)
        
        embeddings = None
        if BedrockEmbeddings:
            try:
                embeddings = BedrockEmbeddings(
                    client=bedrock_runtime_client,
                    region_name=settings.AWS_REGION,
                    model_id="cohere.embed-v4:0" # cohere.embed-multilingual-v3.0
                )
            except Exception as e:
                st.warning(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        else:
            st.warning("BedrockEmbeddingsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì„ë² ë”© ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        return bedrock_runtime_client, embeddings, s3_client
    except Exception as e:
        st.error(f"AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None, None

# --- S3 ê´€ë ¨ í•¨ìˆ˜ ---

def upload_to_s3(file, s3_client, key):
    """íŒŒì¼ì„ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        s3_client.upload_fileobj(file, settings.S3_BUCKET_NAME, key)
        return True
    except Exception as e:
        st.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False

def delete_file_from_s3(s3_client, s3_key):
    """S3ì—ì„œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        s3_client.delete_object(Bucket=settings.S3_BUCKET_NAME, Key=s3_key)
        return True
    except Exception as e:
        st.error(f"S3 íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False

# --- ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ---

def process_pdf_from_s3(s3_client, key, engine, school_id, embeddings=None):
    """S3ì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ PostgreSQL DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            s3_client.download_fileobj(settings.S3_BUCKET_NAME, key, tmp_file)
            tmp_path = tmp_file.name
        
        pdf_loader = PyPDFLoader(tmp_path)
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n", chunk_size=800, chunk_overlap=100
        )
        documents = pdf_loader.load_and_split(text_splitter=splitter)
        
        with engine.connect() as conn:
            source_url = f"s3://{settings.S3_BUCKET_NAME}/{key}"
            file_name = key.split('/')[-1]

            existing_doc = conn.execute(text("""
                SELECT id FROM documents WHERE source_url = :source_url OR (file_name = :file_name AND school_id = :school_id)
            """), {"source_url": source_url, "file_name": file_name, "school_id": school_id}).fetchone()

            if existing_doc:
                document_id = existing_doc[0]
                conn.execute(text("UPDATE documents SET processed = TRUE, chunks_count = :chunks_count WHERE id = :id"),
                             {"chunks_count": len(documents), "id": document_id})
            else:
                result = conn.execute(text("""
                    INSERT INTO documents (school_id, file_name, source_url, category, processed, chunks_count)
                    VALUES (:school_id, :file_name, :source_url, 'pdf', TRUE, :chunks_count) RETURNING id
                """), {"school_id": school_id, "file_name": file_name, "source_url": source_url, "chunks_count": len(documents)}).fetchone()[0]
                document_id = result

            conn.execute(text("DELETE FROM document_chunks WHERE document_id = :document_id"), {"document_id": document_id})

            for doc in documents:
                embedding_vector = embeddings.embed_query(doc.page_content) if embeddings else None
                conn.execute(text("""
                    INSERT INTO document_chunks (document_id, chunk_text, embedding)
                    VALUES (:document_id, :chunk_text, :embedding)
                """), {"document_id": document_id, "chunk_text": doc.page_content, "embedding": embedding_vector})
            
            conn.commit()
        
        os.unlink(tmp_path)
        return len(documents)
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0

def process_rss_feed(engine, rss_url, school_id, embeddings=None):
    """RSS í”¼ë“œë¥¼ ì²˜ë¦¬í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        feed = feedparser.parse(rss_url)
        chunks_processed = 0
        skipped_duplicates = 0
        
        with engine.connect() as conn:
            feed_title = feed.feed.get('title', rss_url)
            rss_feed_result = conn.execute(text("""
                INSERT INTO rss_feeds (school_id, url, title, status)
                VALUES (:school_id, :url, :title, 'active')
                ON CONFLICT (school_id, url) DO UPDATE SET title = EXCLUDED.title, last_processed = NOW()
                RETURNING id
            """),
            {"school_id": school_id, "url": rss_url, "title": feed_title}).fetchone()
            
            rss_feed_id = rss_feed_result[0]
            
            existing_doc = conn.execute(text("""
                SELECT id FROM documents WHERE source_url = :rss_url AND category = 'rss' AND school_id = :school_id
            """),
            {"rss_url": rss_url, "school_id": school_id}).fetchone()
            
            document_id = existing_doc[0] if existing_doc else conn.execute(text("""
                INSERT INTO documents (school_id, source_url, category, processed, chunks_count)
                VALUES (:school_id, :rss_url, 'rss', FALSE, 0) RETURNING id
            """),
            {"school_id": school_id, "rss_url": rss_url}).fetchone()[0]
            
            existing_contents = conn.execute(text("SELECT chunk_text FROM document_chunks WHERE document_id = :id"), {"id": document_id}).fetchall()
            existing_titles = {line.replace('ì œëª©:', '').strip() for row in existing_contents for line in row[0].split('\n') if line.strip().startswith('ì œëª©:')}
            existing_links = {line.replace('ë§í¬:', '').strip() for row in existing_contents for line in row[0].split('\n') if line.strip().startswith('ë§í¬:')}

            for entry in feed.entries:
                entry_title = entry.get('title', '').strip()
                entry_link = entry.get('link', '').strip()

                if entry_title in existing_titles or entry_link in existing_links:
                    skipped_duplicates += 1
                    continue
                
                content = f"ì œëª©: {entry_title}\në‚´ìš©: {entry.get('summary', '')}\në§í¬: {entry_link}\në°œí–‰ì¼: {entry.get('published', '')}"
                
                splitter = CharacterTextSplitter.from_tiktoken_encoder(separator="\n", chunk_size=800, chunk_overlap=100)
                chunks = splitter.split_text(content)
                
                for chunk in chunks:
                    embedding_vector = embeddings.embed_query(chunk) if embeddings else None
                    conn.execute(text("""
                        INSERT INTO document_chunks (document_id, chunk_text, embedding)
                        VALUES (:doc_id, :chunk, :vec)
                    """),
                    {"doc_id": document_id, "chunk": chunk, "vec": embedding_vector})
                    chunks_processed += 1
                
                existing_titles.add(entry_title)
                existing_links.add(entry_link)

            total_chunks = conn.execute(text("SELECT COUNT(*) FROM document_chunks WHERE document_id = :id"), {"id": document_id}).fetchone()[0]
            conn.execute(text("UPDATE documents SET processed = TRUE, chunks_count = :count WHERE id = :id"), {"count": total_chunks, "id": document_id})
            conn.execute(text("UPDATE rss_feeds SET last_processed = NOW(), processed_count = :count WHERE id = :id"), {"count": total_chunks, "id": rss_feed_id})
            conn.commit()
        
        if skipped_duplicates > 0:
            st.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì‹ ê·œ {chunks_processed}ê°œ ì²­í¬ ì¶”ê°€, ì¤‘ë³µ {skipped_duplicates}ê°œ í•­ëª© ìŠ¤í‚µ")
        
        return chunks_processed
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0
