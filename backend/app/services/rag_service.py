from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import text, func
from typing import List, Optional, Dict, Any, Tuple
import hashlib
import json
import logging
from datetime import datetime, timedelta
from app.services.llm_service import llm_service
from app.database import models

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê²€ìƒ‰ ì„±ëŠ¥ ì„¤ì •
SIMILARITY_THRESHOLDS = {
    "high": 0.85,      # ë§¤ìš° ìœ ì‚¬
    "medium": 0.75,    # ì¼ë°˜ì ìœ¼ë¡œ ìœ ì‚¬ 
    "low": 0.65        # ìµœì†Œ ìœ ì‚¬ë„
}

DISTANCE_THRESHOLDS = {
    "high": 0.3,       # ë§¤ìš° ê°€ê¹Œì›€
    "medium": 0.6,     # ì¼ë°˜ì ìœ¼ë¡œ ê°€ê¹Œì›€
    "low": 0.9         # ìµœì†Œ ê±°ë¦¬
}

# ìºì‹œ ì„¤ì •
CACHE_TTL_MINUTES = 30
MAX_CACHE_SIZE = 1000

class SearchCache:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ê°„ë‹¨í•œ ê²€ìƒ‰ ìºì‹œ"""
    
    def __init__(self):
        self.cache = {}
        self.timestamps = {}
        
    def _generate_key(self, question: str, school_id: int) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{question}:{school_id}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, question: str, school_id: int) -> Optional[Dict]:
        """ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        key = self._generate_key(question, school_id)
        
        if key in self.cache:
            timestamp = self.timestamps.get(key)
            if timestamp and datetime.now() - timestamp < timedelta(minutes=CACHE_TTL_MINUTES):
                logger.info(f"ğŸš€ Cache hit for question: {question[:30]}...")
                return self.cache[key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                self._remove(key)
        
        return None
    
    def set(self, question: str, school_id: int, result: Dict):
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œì— ì €ì¥"""
        key = self._generate_key(question, school_id)
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.cache) >= MAX_CACHE_SIZE:
            self._cleanup_old_entries()
        
        self.cache[key] = result
        self.timestamps[key] = datetime.now()
        logger.info(f"ğŸ’¾ Cached result for question: {question[:30]}...")
    
    def _remove(self, key: str):
        """íŠ¹ì • ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        self.cache.pop(key, None)
        self.timestamps.pop(key, None)
    
    def _cleanup_old_entries(self):
        """ì˜¤ë˜ëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        current_time = datetime.now()
        expired_keys = []
        
        for key, timestamp in self.timestamps.items():
            if current_time - timestamp > timedelta(minutes=CACHE_TTL_MINUTES):
                expired_keys.append(key)
        
        for key in expired_keys:
            self._remove(key)
        
        logger.info(f"ğŸ§¹ Cleaned up {len(expired_keys)} expired cache entries")

class RagResponse(BaseModel):
    """RAG ì‘ë‹µ ëª¨ë¸"""
    answer: str
    source_documents: List[Dict[str, Any]]
    confidence_score: Optional[float] = None
    search_strategy: Optional[str] = None
    fallback_used: bool = False
    category: Optional[str] = None
    cache_hit: bool = False

class HybridRAGService:
    """
    ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG ì„œë¹„ìŠ¤
    - í‚¤ì›Œë“œ í•„í„°ë§ + ê¸°ì¡´ ì„ë² ë”© ì¬í™œìš©
    - ìºì‹±, ê²€ìƒ‰ ì „ëµ, ì¬ë­í‚¹ í¬í•¨
    """
    
    def __init__(self):
        self.llm = llm_service
        self.cache = SearchCache()
        self.search_stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "keyword_searches": 0,
            "vector_searches": 0,
            "hybrid_searches": 0,
            "successful_retrievals": 0,
            "fallback_used": 0
        }
    
    async def get_rag_response(self, question: str, school_id: int, db: Session) -> RagResponse:
        """
        ë©”ì¸ RAG í”„ë¡œì„¸ìŠ¤: í‚¤ì›Œë“œ + ê¸°ì¡´ ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        """
        logger.info(f"ğŸš€ Starting Hybrid RAG process for: '{question[:50]}...'")
        self.search_stats["total_queries"] += 1
        
        try:
            # 1. ìºì‹œ í™•ì¸
            cached_result = self.cache.get(question, school_id)
            if cached_result:
                self.search_stats["cache_hits"] += 1
                cached_result["cache_hit"] = True
                return RagResponse(**cached_result)
            
            # 2. ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ê²°ì •
            search_strategy = await self._determine_search_strategy(question)
            logger.info(f"ğŸ¯ Search strategy: {search_strategy}")
            
            # 3. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(question)
            logger.info(f"ğŸ“ Extracted keywords: {keywords}")
            
            # 4. í‚¤ì›Œë“œ í›„ë³´ í•„í„°ë§
            candidates = await self._find_keyword_candidates(keywords, school_id, db)
            
            if not candidates:
                logger.info("âŒ No keyword candidates found, using fallback")
                return await self._fallback_response(question, school_id, db, "no_candidates")
            
            self.search_stats["keyword_searches"] += 1
            logger.info(f"âœ… Found {len(candidates)} keyword candidates")
            
            # 5. ê¸°ì¤€ ì„ë² ë”© ì„ íƒ ë° ë²¡í„° ê²€ìƒ‰
            reference_candidate = self._select_reference_candidate(candidates, keywords, question)
            search_results = await self._hybrid_vector_search(
                reference_candidate, school_id, db, search_strategy
            )
            
            if not search_results:
                logger.info("âŒ Vector search failed, using best keyword candidate")
                return await self._generate_response_from_candidate(reference_candidate, question, "keyword_only")
            
            self.search_stats["vector_searches"] += 1
            logger.info(f"âœ… Vector search found {len(search_results)} results")
            
            # 6. ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
            if not await self._evaluate_search_quality(search_results, search_strategy):
                logger.info("âš ï¸ Search quality insufficient, using keyword candidate")
                return await self._generate_response_from_candidate(reference_candidate, question, "quality_fallback")
            
            # 7. ê²°ê³¼ ì¬ë­í‚¹
            ranked_results = await self._rerank_results(question, search_results, keywords)
            
            # 8. ìµœì¢… ì‘ë‹µ ìƒì„±
            response = await self._generate_enhanced_response(question, ranked_results, search_strategy)
            
            # 9. ê²°ê³¼ ìºì‹±
            response_dict = response.dict()
            self.cache.set(question, school_id, response_dict)
            
            self.search_stats["successful_retrievals"] += 1
            return response
            
        except Exception as e:
            logger.error(f"Hybrid RAG process failed: {e}")
            return await self._fallback_response(question, school_id, db, "error")
    
    async def _determine_search_strategy(self, question: str) -> str:
        """ì§ˆë¬¸ ë¶„ì„ì„ í†µí•œ ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        question_lower = question.lower()
        
        # ê·œì¹™ ê¸°ë°˜ ì „ëµ ê²°ì •
        if any(word in question_lower for word in ["ì–¸ì œ", "ê¸°ê°„", "ì¼ì •", "ë‚ ì§œ", "ì‹œê°„", "ë§ˆê°"]):
            return "temporal"  # ì‹œê°„ ê´€ë ¨
        elif any(word in question_lower for word in ["ì–´ë””", "ì¥ì†Œ", "ìœ„ì¹˜", "ì–´ëŠ", "ê±´ë¬¼", "í˜¸ì‹¤"]):
            return "spatial"   # ê³µê°„ ê´€ë ¨
        elif any(word in question_lower for word in ["ì–¼ë§ˆ", "ë¹„ìš©", "ê¸ˆì•¡", "ê°€ê²©", "ìˆ˜ìˆ˜ë£Œ", "í¬ìƒê¸ˆ"]):
            return "numerical" # ìˆ˜ì¹˜ ê´€ë ¨
        elif any(word in question_lower for word in ["ì–´ë–»ê²Œ", "ë°©ë²•", "ì ˆì°¨", "ê³¼ì •", "ì‹ ì²­", "ì ‘ìˆ˜"]):
            return "procedural" # ì ˆì°¨ ê´€ë ¨
        elif len(question.split()) <= 3:
            return "keyword"   # ë‹¨ìˆœ í‚¤ì›Œë“œ
        else:
            return "semantic"  # ì˜ë¯¸ ê²€ìƒ‰
    
    def _extract_keywords(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ (í–¥ìƒëœ ë²„ì „)"""
        # ë¶ˆìš©ì–´ ì œê±° (í™•ì¥ë¨)
        stopwords = {
            "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", 
            "ì™€", "ê³¼", "ì˜", "ë„", "ì—ê²Œ", "í•œí…Œ", "ì—ì„œ", "ë¶€í„°", "ê¹Œì§€",
            "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””", "ëˆ„êµ¬", "ì™œ", "ì–´ëŠ", "ì–¼ë§ˆ",
            "ëŒ€í•´", "ê´€í•´", "ëŒ€í•œ", "ê´€í•œ", "ì•Œë ¤", "ë§í•´", "ì„¤ëª…", "ê°€ë¥´ì³",
            "í•´ì£¼ì„¸ìš”", "ì•Œë ¤ì£¼ì„¸ìš”", "ê°€ë¥´ì³ì£¼ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì„¸ìš”", "ì•Œê³ ", "ì‹¶ì–´ìš”",
            "ê¶ê¸ˆ", "í•˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ë˜ë‹¤", "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”", "ë˜ëŠ”"
        }
        
        # ì •ë¦¬ ë° ë¶„ë¦¬
        question_clean = question.replace("?", "").replace(".", "").replace("!", "")
        words = question_clean.split()
        
        # ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ ì„ ë³„ (ê¸¸ì´ ë° ì¤‘ìš”ë„ ê¸°ë°˜)
        keywords = []
        for word in words:
            if len(word) > 1 and word not in stopwords:
                keywords.append(word)
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        priority_keywords = []
        normal_keywords = []
        
        for keyword in keywords:
            # ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ (ë„ë©”ì¸ íŠ¹í™”)
            if any(important in keyword for important in [
                "ê²½ì§„ëŒ€íšŒ", "ì§„ë¡œ", "ì¥í•™", "ì·¨ì—…", "ì¸í„´", "ê¸°ìˆ™ì‚¬", "ë„ì„œê´€",
                "ìˆ˜ê°•", "í•™ì ", "ì„±ì ", "ì¡¸ì—…", "ì‹œí—˜", "ëª¨ì§‘", "ì‹ ì²­"
            ]):
                priority_keywords.append(keyword)
            else:
                normal_keywords.append(keyword)
        
        # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œë¶€í„°, ìµœëŒ€ 5ê°œ
        final_keywords = (priority_keywords + normal_keywords)[:5]
        return final_keywords
    
    async def _find_keyword_candidates(self, keywords: List[str], school_id: int, db: Session) -> List[Dict]:
        """í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ í›„ë³´ ì²­í¬ë“¤ ì°¾ê¸° (í–¥ìƒëœ ë²„ì „)"""
        if not keywords:
            return []
        
        try:
            # í‚¤ì›Œë“œë³„ë¡œ OR ì¡°ê±´ ìƒì„±
            keyword_conditions = []
            for keyword in keywords:
                keyword_conditions.append(f"dc.chunk_text ILIKE '%{keyword}%'")
            
            where_clause = " OR ".join(keyword_conditions)
            
            query = text(f"""
                SELECT 
                    dc.id,
                    dc.chunk_text,
                    dc.embedding,
                    d.file_name,
                    d.source_url,
                    d.department,
                    d.category,
                    d.created_at
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.school_id = :school_id
                  AND ({where_clause})
                  AND dc.embedding IS NOT NULL
                  AND LENGTH(dc.chunk_text) > 50
                ORDER BY LENGTH(dc.chunk_text) DESC, d.created_at DESC
                LIMIT 15
            """)
            
            results = db.execute(query, {"school_id": school_id}).fetchall()
            
            candidates = []
            for result in results:
                # í‚¤ì›Œë“œ ë§¤ì¹˜ ì ìˆ˜ ê³„ì‚°
                keyword_matches = self._count_keyword_matches(result.chunk_text, keywords)
                relevance_score = self._calculate_relevance_score(result.chunk_text, keywords)
                
                candidates.append({
                    "id": result.id,
                    "text": result.chunk_text,
                    "embedding": result.embedding,
                    "file_name": result.file_name,
                    "source_url": result.source_url,
                    "department": result.department,
                    "category": result.category,
                    "created_at": result.created_at,
                    "keyword_matches": keyword_matches,
                    "relevance_score": relevance_score
                })
            
            return candidates
            
        except Exception as e:
            logger.error(f"Failed to find keyword candidates: {e}")
            return []
    
    def _count_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹˜ ê°œìˆ˜ ê³„ì‚°"""
        text_lower = text.lower()
        return sum(1 for keyword in keywords if keyword.lower() in text_lower)
    
    def _calculate_relevance_score(self, text: str, keywords: List[str]) -> float:
        """í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        text_lower = text.lower()
        total_score = 0.0
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in text_lower:
                # í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                weight = len(keyword) / 10.0
                # í…ìŠ¤íŠ¸ ë‚´ ì¶œí˜„ ë¹ˆë„
                frequency = text_lower.count(keyword_lower)
                total_score += weight * frequency
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì •ê·œí™”
        normalized_score = total_score / (len(text) / 1000.0)
        return min(normalized_score, 1.0)
    
    def _select_reference_candidate(self, candidates: List[Dict], keywords: List[str], question: str) -> Dict:
        """ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í›„ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ (í–¥ìƒëœ ì•Œê³ ë¦¬ì¦˜)"""
        if not candidates:
            return None
        
        # ë©€í‹° íŒ©í„° ì ìˆ˜ ê³„ì‚°
        scored_candidates = []
        
        for candidate in candidates:
            score = 0.0
            
            # 1. í‚¤ì›Œë“œ ë§¤ì¹˜ ì ìˆ˜ (40%)
            keyword_score = candidate["keyword_matches"] / len(keywords)
            score += keyword_score * 0.4
            
            # 2. ê´€ë ¨ì„± ì ìˆ˜ (30%)
            relevance_score = candidate["relevance_score"]
            score += relevance_score * 0.3
            
            # 3. í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ (20%)
            text_quality = min(len(candidate["text"]) / 1000.0, 1.0)
            score += text_quality * 0.2
            
            # 4. ìµœì‹ ì„± ì ìˆ˜ (10%)
            if candidate["created_at"]:
                days_old = 30  # ì„ì‹œ ê³ ì •ê°’
                freshness_score = max(0, 1 - (days_old / 365.0))  # 1ë…„ ê¸°ì¤€
                score += freshness_score * 0.1
            
            scored_candidates.append((candidate, score))
        
        # ìµœê³  ì ìˆ˜ í›„ë³´ ì„ íƒ
        best_candidate = max(scored_candidates, key=lambda x: x[1])[0]
        logger.info(f"ğŸ¯ Selected reference: Chunk {best_candidate['id']} (matches: {best_candidate['keyword_matches']})")
        
        return best_candidate
    
    async def _hybrid_vector_search(self, reference_candidate: Dict, school_id: int, db: Session, strategy: str) -> List[tuple]:
        """ê¸°ì¤€ ì„ë² ë”©ì„ ì‚¬ìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not reference_candidate or not reference_candidate["embedding"]:
            return []
        
        try:
            reference_embedding = reference_candidate["embedding"]
            
            # ì „ëµë³„ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°ì •
            limit = self._get_search_limit(strategy)
            
            query = text("""
                SELECT 
                    dc.id,
                    dc.chunk_text,
                    d.file_name,
                    d.source_url,
                    d.department,
                    d.category as doc_category,
                    dc.embedding <=> :reference_embedding as l2_distance,
                    1 - (dc.embedding <=> :reference_embedding) as cosine_similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.school_id = :school_id
                  AND dc.embedding IS NOT NULL
                  AND LENGTH(dc.chunk_text) > 50
                ORDER BY dc.embedding <=> :reference_embedding
                LIMIT :limit
            """)
            
            results = db.execute(query, {
                "reference_embedding": reference_embedding,
                "school_id": school_id,
                "limit": limit
            }).fetchall()
            
            # ê²°ê³¼ í’ˆì§ˆ ë¡œê¹…
            for i, result in enumerate(results[:3]):
                distance = float(result.l2_distance)
                similarity = float(result.cosine_similarity)
                logger.info(f"    ğŸ“„ Result {i+1}: Distance={distance:.4f}, Similarity={similarity:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"Hybrid vector search failed: {e}")
            return []
    
    def _get_search_limit(self, strategy: str) -> int:
        """ê²€ìƒ‰ ì „ëµë³„ ê²°ê³¼ ìˆ˜ ê²°ì •"""
        limits = {
            "keyword": 3,
            "temporal": 5,
            "spatial": 4,
            "numerical": 4,
            "procedural": 6,
            "semantic": 5
        }
        return limits.get(strategy, 5)
    
    async def _evaluate_search_quality(self, search_results: List[tuple], strategy: str) -> bool:
        """ê²€ìƒ‰ ì „ëµë³„ í’ˆì§ˆ í‰ê°€ (ì™„í™”ëœ ê¸°ì¤€)"""
        if not search_results:
            return False
        
        best_result = search_results[0]
        best_distance = float(best_result.l2_distance)
        best_similarity = float(best_result.cosine_similarity)
        
        # ê¸°ì¡´ ì„ë² ë”© ì¬í™œìš© ë°©ì‹ì— ë§ì¶° ì„ê³„ê°’ ì™„í™”
        if strategy == "keyword":
            threshold_type = "low"  # í‚¤ì›Œë“œ ê²€ìƒ‰ì€ ë§¤ìš° ê´€ëŒ€í•˜ê²Œ
        elif strategy in ["temporal", "numerical"]:
            threshold_type = "medium"  # ì •í™•í•œ ì •ë³´ê°€ ì¤‘ìš”í•˜ì§€ë§Œ ì™„í™”
        else:
            threshold_type = "low"  # ì „ë°˜ì ìœ¼ë¡œ ì™„í™”ëœ ê¸°ì¤€
        
        # ì™„í™”ëœ ì„ê³„ê°’
        relaxed_distance_thresholds = {
            "high": 0.5,
            "medium": 0.8,
            "low": 1.2
        }
        
        relaxed_similarity_thresholds = {
            "high": 0.5,
            "medium": 0.3,
            "low": 0.1
        }
        
        distance_ok = best_distance < relaxed_distance_thresholds[threshold_type]
        similarity_ok = best_similarity > relaxed_similarity_thresholds[threshold_type]
        
        logger.info(f"ğŸ¯ Quality check ({threshold_type}): "
              f"Distance={best_distance:.4f} ({'âœ…' if distance_ok else 'âŒ'}), "
              f"Similarity={best_similarity:.4f} ({'âœ…' if similarity_ok else 'âŒ'})")
        
        return distance_ok or similarity_ok
    
    async def _rerank_results(self, question: str, search_results: List[tuple], keywords: List[str]) -> List[tuple]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¬ë­í‚¹ (í‚¤ì›Œë“œ ì¹œí™”ì )"""
        logger.info(f"ğŸ”„ Re-ranking {len(search_results)} results...")
        
        if len(search_results) <= 1:
            return search_results
        
        try:
            scored_results = []
            
            for result in search_results:
                # ê¸°ë³¸ ì ìˆ˜ (ìœ ì‚¬ë„)
                base_score = float(result.cosine_similarity)
                
                # ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚°
                bonus_score = 0.0
                
                # í‚¤ì›Œë“œ ë§¤ì¹˜ ë³´ë„ˆìŠ¤ (ì¤‘ìš”!)
                keyword_matches = self._count_keyword_matches(result.chunk_text, keywords)
                if keyword_matches > 0:
                    keyword_bonus = min(keyword_matches * 0.1, 0.3)  # ìµœëŒ€ 30% ë³´ë„ˆìŠ¤
                    bonus_score += keyword_bonus
                
                # ë¶€ì„œ ì •ë³´ ë³´ë„ˆìŠ¤
                if hasattr(result, 'department') and result.department:
                    bonus_score += 0.05
                
                # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³´ë„ˆìŠ¤
                if len(result.chunk_text) > 100:
                    bonus_score += 0.02
                
                # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
                if hasattr(result, 'doc_category'):
                    category_bonus = self._calculate_category_bonus(question, result.doc_category)
                    bonus_score += category_bonus
                
                final_score = base_score + bonus_score
                scored_results.append((result, final_score))
            
            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
            scored_results.sort(key=lambda x: x[1], reverse=True)
            ranked_results = [result for result, score in scored_results]
            
            logger.info("âœ… Re-ranking complete")
            return ranked_results
            
        except Exception as e:
            logger.warning(f"Re-ranking failed, using original order: {e}")
            return search_results
    
    def _calculate_category_bonus(self, question: str, doc_category: str) -> float:
        """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ì™€ ì§ˆë¬¸ ê°„ì˜ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤"""
        question_lower = question.lower()
        
        category_keywords = {
            "career": ["ì§„ë¡œ", "ì·¨ì—…", "ì¸í„´", "ê²½ì§„ëŒ€íšŒ", "ì±„ìš©"],
            "scholarship": ["ì¥í•™", "í•™ë¹„", "ë“±ë¡ê¸ˆ"],
            "academic": ["í•™ì‚¬", "ìˆ˜ê°•", "í•™ì ", "ì„±ì ", "ì¡¸ì—…"],
            "announcement": ["ê³µì§€", "ì•ˆë‚´", "ëª¨ì§‘", "ì‹ ì²­"]
        }
        
        if doc_category in category_keywords:
            keywords = category_keywords[doc_category]
            matches = sum(1 for keyword in keywords if keyword in question_lower)
            return min(matches * 0.02, 0.1)  # ìµœëŒ€ 10% ë³´ë„ˆìŠ¤
        
        return 0.0
    
    async def _generate_enhanced_response(self, question: str, search_results: List[tuple], strategy: str) -> RagResponse:
        """í–¥ìƒëœ ë‹µë³€ ìƒì„± (í‚¤ì›Œë“œ ì¹œí™”ì )"""
        logger.info("ğŸ“ Generating enhanced response...")
        
        if not search_results:
            return await self._fallback_response(question, 1, None, "no_results")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë” í’ë¶€í•˜ê²Œ)
        context_parts = []
        source_documents = []
        
        for i, result in enumerate(search_results):
            chunk_text = result.chunk_text
            source_info = f"ì¶œì²˜: {result.file_name or result.source_url or 'ë‚´ë¶€ ë¬¸ì„œ'}"
            
            if result.department:
                source_info += f" (ë‹´ë‹¹: {result.department})"
            
            context_parts.append(f"ë¬¸ì„œ {i+1}: {chunk_text}\n{source_info}")
            
            source_documents.append({
                "document_id": getattr(result, 'document_id', result.id),
                "chunk_id": result.id,
                "text": chunk_text[:300] + "..." if len(chunk_text) > 300 else chunk_text,
                "source": result.file_name or result.source_url,
                "department": result.department,
                "category": result.doc_category,
                "similarity_score": float(result.cosine_similarity),
                "distance": float(result.l2_distance),
                "rank": i + 1
            })
        
        context = "\n\n".join(context_parts)
        
        # LLMì„ í†µí•œ ë‹µë³€ ìƒì„±
        final_answer = self.llm.get_chat_response(context, question)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (ì™„í™”ëœ ê¸°ì¤€)
        confidence_score = min(float(search_results[0].cosine_similarity) + 0.2, 1.0)
        
        return RagResponse(
            answer=final_answer,
            source_documents=source_documents,
            confidence_score=confidence_score,
            search_strategy="hybrid_keyword_vector",
            fallback_used=False,
            cache_hit=False
        )
    
    async def _generate_response_from_candidate(self, candidate: Dict, question: str, strategy: str) -> RagResponse:
        """í‚¤ì›Œë“œ í›„ë³´ë¡œë¶€í„° ì§ì ‘ ì‘ë‹µ ìƒì„±"""
        if not candidate:
            return await self._fallback_response(question, 1, None, "no_candidate")
        
        logger.info(f"ğŸ“ Generating response from keyword candidate: {candidate['id']}")
        
        answer = self.llm.get_chat_response(candidate["text"], question)
        
        source_documents = [{
            "document_id": candidate["id"],
            "chunk_id": candidate["id"],
            "text": candidate["text"][:300] + "..." if len(candidate["text"]) > 300 else candidate["text"],
            "source": candidate["file_name"] or candidate["source_url"],
            "department": candidate["department"],
            "category": candidate["category"],
            "keyword_matches": candidate["keyword_matches"],
            "relevance_score": candidate["relevance_score"],
            "rank": 1
        }]
        
        return RagResponse(
            answer=answer,
            source_documents=source_documents,
            confidence_score=0.7,
            search_strategy=strategy,
            fallback_used=False,
            cache_hit=False
        )
    
    async def _fallback_response(self, question: str, school_id: int, db: Session, reason: str) -> RagResponse:
        """í–¥ìƒëœ Fallback ë¡œì§"""
        logger.info(f"ğŸš¨ Fallback triggered: {reason}")
        self.search_stats["fallback_used"] += 1
        
        # ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = self.llm.get_query_category(question)
        logger.info(f"ğŸ“Š Question categorized as: {category}")
        
        # DBì—ì„œ ë‹´ë‹¹ ë¶€ì„œ ì •ë³´ ì¡°íšŒ (ì•ˆì „í•˜ê²Œ)
        contact = None
        if db:
            try:
                contact = db.query(models.DefaultContact).filter(
                    models.DefaultContact.school_id == school_id,
                    models.DefaultContact.category == category
                ).first()
            except:
                pass
        
        if contact:
            answer = self._format_enhanced_fallback_answer(question, category, contact, reason)
        else:
            answer = self._format_generic_fallback_answer(question, category, reason)
        
        return RagResponse(
            answer=answer,
            source_documents=[],
            confidence_score=0.0,
            search_strategy="fallback",
            fallback_used=True,
            category=category,
            cache_hit=False
        )
    
    def _format_enhanced_fallback_answer(self, question: str, category: str, contact, reason: str) -> str:
        """í–¥ìƒëœ Fallback ë‹µë³€ í¬ë§·"""
        category_names = {
            "academic": "í•™ì‚¬",
            "scholarship": "ì¥í•™ê¸ˆ", 
            "facilities": "ì‹œì„¤",
            "career": "ì§„ë¡œ",
            "other": "ê¸°íƒ€"
        }
        
        category_kr = category_names.get(category, "ê´€ë ¨")
        
        return f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í˜„ì¬ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ë‹´ë‹¹ ë¶€ì„œ ì•ˆë‚´

**ğŸ¢ {contact.department}**
- ğŸ“ ì—°ë½ì²˜: **{contact.contact_info or 'ì§ì ‘ ë°©ë¬¸ ë˜ëŠ” í™ˆí˜ì´ì§€ í™•ì¸'}**
- ğŸ“‹ ë‹´ë‹¹ì—…ë¬´: {contact.description or f'{category_kr} ê´€ë ¨ ì—…ë¬´'}

## ğŸ’¡ ì¶”ê°€ ë„ì›€ë§

**ì¦‰ì‹œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë‹¤ë©´:**
1. ìœ„ ë‹´ë‹¹ ë¶€ì„œì— ì „í™” ë¬¸ì˜
2. í•™êµ í™ˆí˜ì´ì§€ ê³µì§€ì‚¬í•­ í™•ì¸  
3. í•™ê³¼ ì‚¬ë¬´ì‹¤ ë°©ë¬¸
4. í•™ìƒì§€ì›ì„¼í„° ì¢…í•© ìƒë‹´

ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ™"""

    def _format_generic_fallback_answer(self, question: str, category: str, reason: str) -> str:
        """ì¼ë°˜ì ì¸ Fallback ë‹µë³€"""
        return f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í˜„ì¬ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ì¶”ì²œ ë¬¸ì˜ì²˜
- **í•™ìƒì§€ì›ì„¼í„°** (ì¢…í•© ìƒë‹´)  
- **í•´ë‹¹ í•™ê³¼ ì‚¬ë¬´ì‹¤**
- **í•™êµ ëŒ€í‘œ ì „í™”**

## ğŸ’¡ ë‹¤ë¥¸ ë°©ë²•
1. í•™êµ í™ˆí˜ì´ì§€ì—ì„œ ê´€ë ¨ ê³µì§€ì‚¬í•­ í™•ì¸
2. í•™ìƒ í¬í„¸ ì‹œìŠ¤í…œ ê²€ìƒ‰  
3. ë™ê¸°ë‚˜ ì„ ë°°ì—ê²Œ ë¬¸ì˜
4. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸

ë„ì›€ì´ ë˜ì§€ ëª»í•´ ì£„ì†¡í•©ë‹ˆë‹¤. ğŸ™"""
    
    def get_search_stats(self) -> Dict[str, Any]:
        """ìƒì„¸ ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        total = self.search_stats["total_queries"]
        if total == 0:
            return self.search_stats
            
        return {
            **self.search_stats,
            "cache_hit_rate": round((self.search_stats["cache_hits"] / total) * 100, 2),
            "keyword_search_rate": round((self.search_stats["keyword_searches"] / total) * 100, 2),
            "vector_search_rate": round((self.search_stats["vector_searches"] / total) * 100, 2),
            "success_rate": round((self.search_stats["successful_retrievals"] / total) * 100, 2),
            "fallback_rate": round((self.search_stats["fallback_used"] / total) * 100, 2)
        }

# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
hybrid_rag_service = HybridRAGService()

# ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
async def get_rag_response(question: str, school_id: int, db: Session) -> RagResponse:
    """ë©”ì¸ RAG í•¨ìˆ˜ - í‚¤ì›Œë“œ + ê¸°ì¡´ ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ"""
    return await hybrid_rag_service.get_rag_response(question, school_id, db)

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
async def get_search_statistics() -> Dict[str, Any]:
    """ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    return hybrid_rag_service.get_search_stats()

async def clear_search_cache():
    """ê²€ìƒ‰ ìºì‹œ í´ë¦¬ì–´"""
    hybrid_rag_service.cache.cache.clear()
    hybrid_rag_service.cache.timestamps.clear()
    logger.info("ğŸ§¹ Search cache cleared")

if __name__ == "__main__":
    print("=== Hybrid RAG Service v3.0 (Complete) ===")
    print("Features:")
    print("- Keyword filtering + Existing embedding reuse")
    print("- Advanced caching system")
    print("- Multi-factor candidate selection")
    print("- Enhanced re-ranking with keyword bonus")
    print("- Comprehensive search strategies")
    print("- Detailed statistics tracking")